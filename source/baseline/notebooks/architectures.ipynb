{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Architectures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "# Machine learning\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with Kernel Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 32, 64, 128, 256]\n",
      "[8, 16, 32, 64, 128]\n",
      "[4, 8, 16, 32, 64]\n",
      "[2, 4, 8, 16, 32]\n"
     ]
    }
   ],
   "source": [
    "# Kernel sizes on modified OSnet (type 1)\n",
    "# We could re-use the kernel sizes to make up six bottleneck layers\n",
    "for i in range(4):\n",
    "    print([2**n for n in range(4-i, 9-i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 128, 192, 256, 320]\n",
      "[32, 64, 96, 128, 160]\n",
      "[16, 32, 48, 64, 80]\n",
      "[8, 16, 24, 32, 40]\n",
      "[4, 8, 12, 16, 20]\n",
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "# Kernel sizes on modified OSnet (type 2)\n",
    "# With type 1, we don't have any kernels between 128 and 256 (for instance)\n",
    "# type 2 might be better for feature extraction\n",
    "for i in np.arange(1, 7)[::-1]:\n",
    "    print([(2**i)*n for n in range(1, 6)])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using odd-sized kernels symmetrically divide the previous layer output in the current output. \n",
    "In the abscence of this symmetry, systematic distortions of the input occur across layers.\n",
    "Link: https://towardsdatascience.com/deciding-optimal-filter-size-for-cnns-d6f7b56f9363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65, 129, 193, 257, 321]\n",
      "[33, 65, 97, 129, 161]\n",
      "[17, 33, 49, 65, 81]\n",
      "[9, 17, 25, 33, 41]\n",
      "[5, 9, 13, 17, 21]\n",
      "[3, 5, 7, 9, 11]\n"
     ]
    }
   ],
   "source": [
    "# We can choose to make these kernels sizes odd instead\n",
    "# So the minimum kernel size that we will be looking at is 3, not 2\n",
    "# So just, add 1 to all kernels sizes?\n",
    "\n",
    "# Kernel sizes on modified OSnet (type 2)\n",
    "# With type 1, we don't have any kernels between 128 and 256 (for instance)\n",
    "# type 2 might be better for feature extraction\n",
    "for i in np.arange(1, 7)[::-1]:\n",
    "    print([(2**i)*n + 1 for n in range(1, 6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317 is the closest prime kernel size\n"
     ]
    }
   ],
   "source": [
    "# We could also look at using prime number kernel sizes\n",
    "# Refer: https://openreview.net/pdf?id=PDYs7Z2XFGv\n",
    "\n",
    "# The Sieve of Eratosthenes method of calculating the primes less than the limit\n",
    "def getPrimes(limit):\n",
    "    # The list of prime numbers\n",
    "    primes = []\n",
    "    # The boolean list of whether a number is prime\n",
    "    numbers = [True] * limit\n",
    "    # Loop all of the numbers in numbers starting from 2\n",
    "    for i in range(2, limit):\n",
    "        # If the number is prime\n",
    "        if numbers[i]:\n",
    "            # Add it onto the list of prime numbers\n",
    "            primes.append(i)\n",
    "            # Loop over all of the other factors in the list\n",
    "            for n in range(i ** 2, limit, i):\n",
    "                # Make them not prime\n",
    "                numbers[n] = False\n",
    "\n",
    "    # Return the list of prime numbers\n",
    "    return primes\n",
    "\n",
    "def get_closest_prime_kernel_size(kernel_size):\n",
    "    primes = getPrimes(kernel_size + 100)\n",
    "\n",
    "    # The distance away from the closest prime\n",
    "    maxDist = math.inf\n",
    "    # The closest prime\n",
    "    closest_prime = 0\n",
    "\n",
    "    # Loop all of the primes\n",
    "    for p in primes:\n",
    "        # if the prime number is closer than maxDist\n",
    "        if abs(kernel_size - p) < maxDist:\n",
    "            maxDist = abs(kernel_size - p)\n",
    "            closest_prime = p\n",
    "    return closest_prime\n",
    "\n",
    "kernel_size = 320\n",
    "closest_prime = get_closest_prime_kernel_size(kernel_size)\n",
    "# Print the output\n",
    "print(closest_prime, \"is the closest prime kernel size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61, 127, 191, 257, 317]\n",
      "[31, 61, 97, 127, 157]\n",
      "[17, 31, 47, 61, 79]\n",
      "[7, 17, 23, 31, 41]\n",
      "[3, 7, 11, 17, 19]\n",
      "[2, 3, 5, 7, 11]\n"
     ]
    }
   ],
   "source": [
    "# Kernel sizes on modified OSnet (type 2p)\n",
    "# Modifying type 2 to use closest prime kernel size\n",
    "for i in np.arange(1, 7)[::-1]:\n",
    "    kernels = [(2**i)*n for n in range(1, 6)]\n",
    "    prime_kernels = [get_closest_prime_kernel_size(foo) for foo in kernels]\n",
    "    print(prime_kernels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can also set all kernels sizes to be size=3. When using stacking on the OSnet, we should be looking at a wide enough receptive field.\n",
    "A 20 Hz (signal low freq cutoff) signal at 220 Hz sampling frequency contains 11 samples per cycle.\n",
    "We need kernels sizes of 11 and smaller in order to perceive the largest possible feature. \n",
    "Using the (2t+1) relation for calculating receptive field. Stacking 5 layers of kernel size = 3,\n",
    "will give us (2*5 + 1) an effective receptive field of 11.\n",
    "\n",
    "So, we can use 5 different scales in each bottleneck layer with stack size = (1, 2, 3, 4, 5) for 5 different scales.\n",
    "This is assuming that larger scales are not required for detection purposes (loosely, scales = receptive field). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the OSnet Architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Github Link: https://github.com/KaiyangZhou/deep-person-reid/blob/master/torchreid/models/osnet_ain.py#L309"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to Paper: https://arxiv.org/abs/1905.00953"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Major Modifications && Notes:\n",
    "\n",
    "1. Converting OSnet from 2D to 1D (needs to extract features from time series)\n",
    "2. Throwing away layers that don't contribute to our purpose\n",
    "3. Extracting features using large kernels (Link: https://openaccess.thecvf.com/content_cvpr_2017/papers/Peng_Large_Kernel_Matters_CVPR_2017_paper.pdf)\n",
    "4. Making the network fully convolutional (no fully connected layers) and removing global pooling layers (both localisation and classification is better)\n",
    "5. Using a more stable activation function (getting rid of ReLU -> probably Leaky ReLU, SiLU or GELU)\n",
    "6. Input has been manufactured to be exactly 4096 samples in length. Set strides to get it down to 128 (x32 compression).\n",
    "7. 20 Hz (signal low freq cutoff) at 220 Hz sampling freq will require at least kernel size = 11 to capture one full cycle.\n",
    "8. AveragePool or MaxPool or both (currently this in OSnet)? Maybe even MinPool for funsies.\n",
    "   Thinking about it more: MaxPool gets bright features from darker images. MinPool should do the opposite.\n",
    "   What if MinPool actually helps in gathering low-SNR features? Maybe try it for realsies."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "class ConvLayer(nn.Module):\n",
    "    \"\"\"Convolution layer (conv + bn + relu).\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        groups=1,\n",
    "        IN=False\n",
    "    ):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=False,\n",
    "            groups=groups\n",
    "        )\n",
    "        if IN:\n",
    "            self.bn = nn.InstanceNorm1d(out_channels, affine=True)\n",
    "        else:\n",
    "            self.bn = nn.BatchNorm1d(out_channels)\n",
    "        self.silu = nn.SiLU() # Using the swish function instead of ReLU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return self.silu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "class Conv1x1(nn.Module):\n",
    "    \"\"\"1x1 convolution + bn + relu.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, groups=1):\n",
    "        super(Conv1x1, self).__init__()\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=stride,\n",
    "            padding=0,\n",
    "            bias=False,\n",
    "            groups=groups\n",
    "        )\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "        self.silu = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return self.silu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "class Conv1x1Linear(nn.Module):\n",
    "    \"\"\"1x1 convolution + bn (w/o non-linearity).\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, bn=True):\n",
    "        super(Conv1x1Linear, self).__init__()\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size=1, \n",
    "            stride=stride, \n",
    "            padding=0, \n",
    "            bias=False\n",
    "        )\n",
    "        self.bn = None\n",
    "        if bn:\n",
    "            self.bn = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "class LightConvNxN(nn.Module):\n",
    "    \"\"\"Lightweight NxN convolution.\n",
    "\n",
    "    1x1 (linear) + dw NxN (nonlinear).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(LightConvNxN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels, out_channels, 1, stride=1, padding=0, bias=False\n",
    "        )\n",
    "        # Before applying kernel size, use padding=same if required\n",
    "        padding = 'same' if kernel_size != 3 else 1\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=padding,\n",
    "            bias=False,\n",
    "            groups=out_channels\n",
    "        )\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "        self.silu = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn(x)\n",
    "        return self.silu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "class LightConvStream(nn.Module):\n",
    "    \"\"\"Lightweight convolution stream. Stacking conv layers for larger receptive field.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, depth, kernel_size):\n",
    "        super(LightConvStream, self).__init__()\n",
    "        assert depth >= 1, 'depth must be equal to or larger than 1, but got {}'.format(\n",
    "            depth\n",
    "        )\n",
    "        layers = []\n",
    "        layers += [LightConvNxN(in_channels, out_channels, kernel_size)]\n",
    "        for i in range(depth - 1):\n",
    "            layers += [LightConvNxN(out_channels, out_channels, kernel_size)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building blocks for omni-scale feature learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "class ChannelGate(nn.Module):\n",
    "    \"\"\"A mini-network that generates channel-wise gates conditioned on input tensor.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        num_gates=None,\n",
    "        return_gates=False,\n",
    "        gate_activation='sigmoid',\n",
    "        reduction=8\n",
    "    ):\n",
    "        super(ChannelGate, self).__init__()\n",
    "        if num_gates is None:\n",
    "            num_gates = in_channels\n",
    "        self.return_gates = return_gates\n",
    "        # Global average pooling here is only used to get channel-wise weights\n",
    "        # Each scale learnt is given an individual weight\n",
    "        # The vector of weights is then multiplied with the different scales to get weighted scales\n",
    "        # This does not get rid of localisation features in any way\n",
    "        self.global_avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc1 = nn.Conv1d(\n",
    "            in_channels,\n",
    "            in_channels // reduction,\n",
    "            kernel_size=1,\n",
    "            bias=True,\n",
    "            padding=0\n",
    "        )\n",
    "        self.norm1 = None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.silu = nn.SiLU()\n",
    "        self.fc2 = nn.Conv1d(\n",
    "            in_channels // reduction,\n",
    "            num_gates,\n",
    "            kernel_size=1,\n",
    "            bias=True,\n",
    "            padding=0\n",
    "        )\n",
    "        if gate_activation == 'sigmoid':\n",
    "            self.gate_activation = nn.Sigmoid()\n",
    "        elif gate_activation == 'relu':\n",
    "            self.gate_activation = nn.ReLU()\n",
    "        elif gate_activation == 'silu':\n",
    "            self.gate_activation = nn.SiLU()\n",
    "        elif gate_activation == 'linear':\n",
    "            self.gate_activation = None\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                \"Unknown gate activation: {}\".format(gate_activation)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        x = self.global_avgpool(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.fc2(x)\n",
    "        if self.gate_activation is not None:\n",
    "            x = self.gate_activation(x)\n",
    "        if self.return_gates:\n",
    "            return x\n",
    "        # Each scale should now be weighted\n",
    "        return input * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "class OSBlock(nn.Module):\n",
    "    \"\"\"Omni-scale feature learning block.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_sizes, reduction=4, T=5, stacking=False, **kwargs):\n",
    "        super(OSBlock, self).__init__()\n",
    "        assert T >= 1\n",
    "        assert out_channels >= reduction and out_channels % reduction == 0\n",
    "        mid_channels = out_channels // reduction\n",
    "\n",
    "        self.conv1 = Conv1x1(in_channels, mid_channels)\n",
    "        # Stacking conv layers for increased receptive field and low cost\n",
    "        # We can either use non-stacked large kernels or stacked small kernels\n",
    "        # k=3 with T=5 gives an effective receptive field of (2*5+1=11)\n",
    "        self.conv2 = nn.ModuleList()\n",
    "        if stacking:\n",
    "            # Iterating through different scales\n",
    "            for n, t in enumerate(range(1, T + 1)):\n",
    "                self.conv2 += [LightConvStream(mid_channels, mid_channels, t, kernel_sizes[n])]\n",
    "        else:\n",
    "            # Using larger kernel sizes without stacking\n",
    "            for kernel_size in kernel_sizes:\n",
    "                self.conv2 += [LightConvNxN(mid_channels, mid_channels, kernel_size)]\n",
    "        \n",
    "        self.gate = ChannelGate(mid_channels)\n",
    "        self.conv3 = Conv1x1Linear(mid_channels, out_channels)\n",
    "        self.downsample = None\n",
    "        if in_channels != out_channels:\n",
    "            self.downsample = Conv1x1Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = 0\n",
    "        for conv2_t in self.conv2:\n",
    "            x2_t = conv2_t(x1)\n",
    "            x2 = x2 + self.gate(x2_t)\n",
    "        x3 = self.conv3(x2)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(identity)\n",
    "        out = x3 + identity\n",
    "        return F.silu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "class OSBlockINin(nn.Module):\n",
    "    \"\"\"Omni-scale feature learning block with instance normalization.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_sizes, reduction=4, T=5, stacking=False, **kwargs):\n",
    "        super(OSBlockINin, self).__init__()\n",
    "        assert T >= 1\n",
    "        assert out_channels >= reduction and out_channels % reduction == 0\n",
    "        mid_channels = out_channels // reduction\n",
    "\n",
    "        self.conv1 = Conv1x1(in_channels, mid_channels)\n",
    "        # Stacking conv layers for increased receptive field and low cost\n",
    "        # We can either use non-stacked large kernels or stacked small kernels\n",
    "        # k=3 with T=5 gives an effective receptive field of (2*5+1=11)\n",
    "        self.conv2 = nn.ModuleList()\n",
    "        if stacking:\n",
    "            # Iterating through different scales\n",
    "            for n, t in enumerate(range(1, T + 1)):\n",
    "                self.conv2 += [LightConvStream(mid_channels, mid_channels, t, kernel_sizes[n])]\n",
    "        else:\n",
    "            # Using larger kernel sizes without stacking\n",
    "            for kernel_size in kernel_sizes:\n",
    "                self.conv2 += [LightConvStream(mid_channels, mid_channels, 1, kernel_size)]\n",
    "        \n",
    "        self.gate = ChannelGate(mid_channels)\n",
    "        self.conv3 = Conv1x1Linear(mid_channels, out_channels, bn=False)\n",
    "        self.downsample = None\n",
    "        if in_channels != out_channels:\n",
    "            self.downsample = Conv1x1Linear(in_channels, out_channels)\n",
    "        self.IN = nn.InstanceNorm1d(out_channels, affine=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = 0\n",
    "        for conv2_t in self.conv2:\n",
    "            x2_t = conv2_t(x1)\n",
    "            x2 = x2 + self.gate(x2_t)\n",
    "        x3 = self.conv3(x2)\n",
    "        x3 = self.IN(x3) # IN inside residual\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(identity)\n",
    "        out = x3 + identity\n",
    "        return F.relu(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "class OSNet(nn.Module):\n",
    "    \"\"\"Omni-Scale Network.\n",
    "    \n",
    "    Reference:\n",
    "        - Zhou et al. Omni-Scale Feature Learning for Person Re-Identification. ICCV, 2019.\n",
    "        - Zhou et al. Learning Generalisable Omni-Scale Representations\n",
    "          for Person Re-Identification. TPAMI, 2021.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        blocks,\n",
    "        layers,\n",
    "        channels,\n",
    "        kernel_sizes,\n",
    "        strides,\n",
    "        conv1_IN=False,\n",
    "        in_channels=1,\n",
    "        stacking=True,\n",
    "        initial_dim_reduction = False\n",
    "    ):\n",
    "        super(OSNet, self).__init__()\n",
    "        num_blocks = len(blocks)\n",
    "        assert num_blocks == len(layers)\n",
    "        assert num_blocks == len(channels) - 1\n",
    "\n",
    "        # options\n",
    "        self.initial_dim_reduction = initial_dim_reduction\n",
    "\n",
    "        # convolutional backbone\n",
    "        self.conv1 = ConvLayer(\n",
    "            in_channels, channels[0], 7, stride=strides[0], padding=3, IN=conv1_IN\n",
    "        )\n",
    "        self.maxpool = nn.MaxPool1d(3, stride=strides[1], padding=1)\n",
    "\n",
    "        ## OSnet bottlenecks and dimensionality reduction\n",
    "        # conv2 = bottleneck x2\n",
    "        if not self.initial_dim_reduction:\n",
    "            channels_init = in_channels\n",
    "        else:\n",
    "            channels_init = channels[0]\n",
    "        self.conv2 = self._make_layer(\n",
    "            blocks[0], layers[0], kernel_sizes[0], channels_init, channels[1], stacking\n",
    "        )\n",
    "        # pool2 = 1x1 conv + 2x2 avg pool + !!!stride 2!!!\n",
    "        # Length of the array reduced by x2\n",
    "        self.pool2 = nn.Sequential(\n",
    "            Conv1x1(channels[1], channels[1]), nn.AvgPool1d(2, stride=strides[2])\n",
    "        )\n",
    "\n",
    "        # conv3 = bottleneck x2\n",
    "        self.conv3 = self._make_layer(\n",
    "            blocks[1], layers[1], kernel_sizes[1], channels[1], channels[2], stacking\n",
    "        )\n",
    "        # pool3 = 1x1 conv + 2x2 avg pool + !!!stride 2!!!\n",
    "        # Length of the array reduced by x4\n",
    "        self.pool3 = nn.Sequential(\n",
    "            Conv1x1(channels[2], channels[2]), nn.AvgPool1d(2, stride=strides[3])\n",
    "        )\n",
    "\n",
    "        # conv4 = bottleneck x2\n",
    "        self.conv4 = self._make_layer(\n",
    "            blocks[2], layers[2], kernel_sizes[2], channels[2], channels[3], stacking\n",
    "        )\n",
    "        self.conv5 = Conv1x1(channels[3], channels[3])\n",
    "        \n",
    "        self._init_params()\n",
    "\n",
    "    def _make_layer(self, blocks, layer, kernel_sizes, in_channels, out_channels, stacking):\n",
    "        # I'm guessing layer variable is not used here because it's always (2,2,2)\n",
    "        layers = []\n",
    "        layers += [blocks[0](in_channels, out_channels, kernel_sizes=kernel_sizes[0], stacking=stacking)]\n",
    "        for i in range(1, len(blocks)):\n",
    "            layers += [blocks[i](out_channels, out_channels, kernel_sizes=kernel_sizes[i], stacking=stacking)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _init_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_out', nonlinearity='relu'\n",
    "                )\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "            elif isinstance(m, nn.InstanceNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def featuremaps(self, x):\n",
    "        if self.initial_dim_reduction:\n",
    "            x = self.conv1(x)\n",
    "            x = self.maxpool(x)\n",
    "        # Rest of OSnet\n",
    "        print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        print(x.shape)\n",
    "        x = self.pool2(x)\n",
    "        print(x.shape)\n",
    "        x = self.conv3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x).unsqueeze(1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feature maps is all we need from frontend\n",
    "        return self.featuremaps(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OSnet with different configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def osnet_ain_x1_0(kernel_sizes, strides, stacking=True, initial_dim_reduction=False):\n",
    "    model = OSNet(\n",
    "        blocks=[\n",
    "            [OSBlockINin, OSBlockINin], [OSBlock, OSBlockINin],\n",
    "            [OSBlockINin, OSBlock]\n",
    "        ],\n",
    "        layers=[2, 2, 2],\n",
    "        channels=[64, 256, 384, 512],\n",
    "        conv1_IN=True,\n",
    "        kernel_sizes=kernel_sizes,\n",
    "        strides=strides,\n",
    "        stacking=stacking,\n",
    "        initial_dim_reduction = initial_dim_reduction\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def osnet_ain_x0_75(kernel_sizes, strides, stacking=True, initial_dim_reduction=False):\n",
    "    model = OSNet(\n",
    "        blocks=[\n",
    "            [OSBlockINin, OSBlockINin], [OSBlock, OSBlockINin],\n",
    "            [OSBlockINin, OSBlock]\n",
    "        ],\n",
    "        layers=[2, 2, 2],\n",
    "        channels=[48, 192, 288, 384],\n",
    "        conv1_IN=True,\n",
    "        kernel_sizes=kernel_sizes,\n",
    "        strides=strides,\n",
    "        stacking=stacking,\n",
    "        initial_dim_reduction = initial_dim_reduction\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def osnet_ain_x0_5(kernel_sizes, strides, stacking=True, initial_dim_reduction=False):\n",
    "    model = OSNet(\n",
    "        blocks=[\n",
    "            [OSBlockINin, OSBlockINin], [OSBlock, OSBlockINin],\n",
    "            [OSBlockINin, OSBlock]\n",
    "        ],\n",
    "        layers=[2, 2, 2],\n",
    "        channels=[32, 128, 192, 256],\n",
    "        conv1_IN=True,\n",
    "        kernel_sizes=kernel_sizes,\n",
    "        strides=strides,\n",
    "        stacking=stacking,\n",
    "        initial_dim_reduction = initial_dim_reduction\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def osnet_ain_x0_25(kernel_sizes, strides, stacking=True, initial_dim_reduction=False):\n",
    "    model = OSNet(\n",
    "        blocks=[\n",
    "            [OSBlockINin, OSBlockINin], [OSBlock, OSBlockINin],\n",
    "            [OSBlockINin, OSBlock]\n",
    "        ],\n",
    "        layers=[2, 2, 2],\n",
    "        channels=[16, 64, 96, 128],\n",
    "        conv1_IN=True,\n",
    "        kernel_sizes=kernel_sizes,\n",
    "        strides=strides,\n",
    "        stacking=stacking,\n",
    "        initial_dim_reduction = initial_dim_reduction\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def osnet_ain_custom(channels, kernel_sizes, strides, stacking=True, initial_dim_reduction=False):\n",
    "    model = OSNet(\n",
    "        blocks=[\n",
    "            [OSBlockINin, OSBlockINin], [OSBlock, OSBlockINin],\n",
    "            [OSBlockINin, OSBlock]\n",
    "        ],\n",
    "        layers=[2, 2, 2],\n",
    "        channels=channels,\n",
    "        conv1_IN=True,\n",
    "        kernel_sizes=kernel_sizes,\n",
    "        strides=strides,\n",
    "        stacking=stacking,\n",
    "        initial_dim_reduction = initial_dim_reduction\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running OSnet "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NOTES:\n",
    "1. The first two values in strides var is used by initial dim reduction layers\n",
    "2. The last two values in strides var is used by maxpool after bottlenecks\n",
    "3. If using initial dim reduction, all four strides are done. Make sure to set them to proper values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 4096])\n",
      "torch.Size([64, 64, 4096])\n",
      "torch.Size([64, 64, 512])\n",
      "torch.Size([64, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "k_default = [3,3,3,3,3]\n",
    "\n",
    "network = osnet_ain_x0_25(\n",
    "            kernel_sizes=[\n",
    "                [k_default, k_default], [k_default, k_default],\n",
    "                [k_default, k_default]\n",
    "            ],\n",
    "            strides=[2,2,8,4],\n",
    "            stacking=True,\n",
    "            initial_dim_reduction=False\n",
    "        )\n",
    "\n",
    "input = torch.randn(64, 1, 4096)\n",
    "output = network(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65, 129, 193, 257, 321]\n",
      "[33, 65, 97, 129, 161]\n",
      "[17, 33, 49, 65, 81]\n",
      "[9, 17, 25, 33, 41]\n",
      "[5, 9, 13, 17, 21]\n",
      "[3, 5, 7, 9, 11]\n"
     ]
    }
   ],
   "source": [
    "# Kernel sizes on modified OSnet (type 2)\n",
    "# With type 1, we don't have any kernels between 128 and 256 (for instance)\n",
    "# type 2 might be better for feature extraction\n",
    "for i in np.arange(1, 7)[::-1]:\n",
    "    print([(2**i)*n + 1 for n in range(1, 6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 4096])\n",
      "torch.Size([64, 64, 4096])\n",
      "torch.Size([64, 64, 512])\n",
      "torch.Size([64, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Does it work with different kernel sizes?\n",
    "k_check = [65, 129, 193, 257, 321]\n",
    "\n",
    "network = osnet_ain_x0_25(\n",
    "            kernel_sizes=[\n",
    "                [k_check, k_check], [k_check, k_check],\n",
    "                [k_check, k_check]\n",
    "            ],\n",
    "            strides=[2,2,8,4],\n",
    "            stacking=False,\n",
    "            initial_dim_reduction=False\n",
    "        )\n",
    "\n",
    "input = torch.randn(64, 1, 4096)\n",
    "output = network(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4096])\n",
      "torch.Size([1, 64, 4096])\n",
      "torch.Size([1, 64, 512])\n",
      "torch.Size([1, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# All Custom Kernels\n",
    "kernel_sizes = []\n",
    "tmp = []\n",
    "for i in np.arange(1, 7)[::-1]:\n",
    "    tmp.append([(2**i)*n + 1 for n in range(1, 6)])\n",
    "    if i%2!=0:\n",
    "        kernel_sizes.append(tmp)\n",
    "        tmp = []\n",
    "\n",
    "# Without initial dim reduction, num channels starts with channels[1]\n",
    "network = osnet_ain_custom(\n",
    "            channels=[16, 64, 96, 128],\n",
    "            kernel_sizes=kernel_sizes,\n",
    "            strides=[2,2,8,4],\n",
    "            stacking=False,\n",
    "            initial_dim_reduction=False\n",
    "        )\n",
    "\n",
    "input = torch.randn(1, 1, 4096)\n",
    "output = network(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 4096])\n",
      "torch.Size([64, 32, 4096])\n",
      "torch.Size([64, 32, 512])\n",
      "torch.Size([64, 1, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nnarenraju/Research/ORChiD/DEBUGGING/ML-GWSC1-Glasgow/source/baseline/.venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py:306: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1008.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "# Kernel sizes on modified OSnet (type 1)\n",
    "# We could re-use the kernel sizes to make up six bottleneck layers\n",
    "kernel_sizes = []\n",
    "kernel_sizes.append([[16, 32, 64, 128, 256], [8, 16, 32, 64, 128]])\n",
    "kernel_sizes.append([[8, 16, 32, 64, 128], [2, 4, 8, 16, 32]])\n",
    "kernel_sizes.append([[2, 4, 8, 16, 32], [2, 4, 8, 16, 32]])\n",
    "\n",
    "# Without initial dim reduction, num channels starts with channels[1]\n",
    "# We can reduce these number for a smaller network\n",
    "# ChannelGate has a reduction factor which is 32 by default. So channels=32 cannot be used with this.\n",
    "network = osnet_ain_custom(\n",
    "            channels=[16, 32, 64, 128],\n",
    "            kernel_sizes=kernel_sizes,\n",
    "            strides=[2,2,8,4],\n",
    "            stacking=False,\n",
    "            initial_dim_reduction=False\n",
    "        )\n",
    "\n",
    "input = torch.randn(64, 1, 4096)\n",
    "output = network(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 4096])\n",
      "torch.Size([64, 32, 4096])\n",
      "torch.Size([64, 32, 512])\n",
      "torch.Size([64, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Kernel sizes on modified OSnet (type 1odd)\n",
    "# We could re-use the kernel sizes to make up six bottleneck layers\n",
    "kernel_sizes = []\n",
    "kernel_sizes.append([[17, 33, 65, 129, 257], [9, 17, 33, 65, 129]])\n",
    "kernel_sizes.append([[9, 17, 33, 65, 129], [3, 5, 9, 17, 33]])\n",
    "kernel_sizes.append([[3, 5, 9, 17, 33], [3, 5, 9, 17, 33]])\n",
    "\n",
    "network = osnet_ain_custom(\n",
    "            channels=[16, 32, 64, 128],\n",
    "            kernel_sizes=kernel_sizes,\n",
    "            strides=[2,2,8,4],\n",
    "            stacking=False,\n",
    "            initial_dim_reduction=False\n",
    "        )\n",
    "\n",
    "input = torch.randn(64, 1, 4096)\n",
    "output = network(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 512])\n",
      "torch.Size([64, 32, 512])\n",
      "torch.Size([64, 32, 256])\n",
      "torch.Size([64, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Kernel sizes on modified OSnet (type 1odd)\n",
    "# We could re-use the kernel sizes to make up six bottleneck layers\n",
    "kernel_sizes = []\n",
    "kernel_sizes.append([[17, 33, 65, 129, 257], [9, 17, 33, 65, 129]])\n",
    "kernel_sizes.append([[9, 17, 33, 65, 129], [3, 5, 9, 17, 33]])\n",
    "kernel_sizes.append([[3, 5, 9, 17, 33], [3, 5, 9, 17, 33]])\n",
    "\n",
    "network = osnet_ain_custom(\n",
    "            channels=[16, 32, 64, 128],\n",
    "            kernel_sizes=kernel_sizes,\n",
    "            strides=[4,2,2,2],\n",
    "            stacking=False,\n",
    "            initial_dim_reduction=True\n",
    "        )\n",
    "\n",
    "input = torch.randn(64, 1, 4096)\n",
    "output = network(input)\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

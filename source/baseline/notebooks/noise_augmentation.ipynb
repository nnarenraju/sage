{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-The-Fly Noise Augmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using O3a/O3b Glitches obtained from Gravity Spy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get glitches from GWOSC using Gravity Spy glitch start times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy import signal\n",
    "\n",
    "from gwpy.timeseries import TimeSeries\n",
    "from pycbc.filter import resample_to_delta_t, highpass\n",
    "from pycbc.types import TimeSeries as TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_length = 17.0 # in seconds\n",
    "low_freq_cutoff = 15.0 # Hz\n",
    "sanity_plot = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(strain, sample_rate=2048., crop=2.5):\n",
    "    res = resample_to_delta_t(strain, 1./sample_rate)\n",
    "    ret = highpass(res, low_freq_cutoff).astype(np.float32)\n",
    "    ret = ret.time_slice(float(ret.start_time) + crop,\n",
    "                         float(ret.end_time) - crop)\n",
    "    return ret\n",
    "\n",
    "def get_glitch_data(args):\n",
    "    try:\n",
    "        idx, csv = args\n",
    "        ref_time = np.random.uniform(0.0, sample_length)\n",
    "        gps = csv['event_time'][idx]\n",
    "        # We pad 2.5 seconds on each side to be removed after downsampling\n",
    "        start = int(gps) - ref_time - 2.5\n",
    "        end = int(gps) + (sample_length - ref_time) + 2.5\n",
    "        #start = int(gps) - 1 - 2.5\n",
    "        #end = int(gps) + H1_O3a['duration'][idx] + 1 + 2.5\n",
    "        glitch = TimeSeries.fetch_open_data(csv['ifo'][idx], start, end, cache=1)\n",
    "        #original_glitch = glitch.value[int(2.5*(1./glitch.dt.value)):int(len(glitch.value)-2.5*(1./glitch.dt.value))]\n",
    "        data = TS(glitch.value, delta_t=glitch.dt.value)\n",
    "        data = downsample(data).numpy()\n",
    "        #dur = sample_length\n",
    "        #time_original = np.linspace(0.0, dur, int(dur*(1./glitch.dt.value)))\n",
    "        #time_downsampled = np.linspace(0.0, dur, int(dur*2048.))\n",
    "        return data\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    cnum = int(sys.argv[1])\n",
    "    limit = int(sys.argv[2])\n",
    "    size = int(sys.argv[3])\n",
    "\n",
    "    chunk_num = cnum\n",
    "    print('chunk num = {}, idx = {} to {}, size = {}'.format(cnum, limit, limit+size, size))\n",
    "    try:\n",
    "        H1_O3a = pd.read_csv('L1_O3b.csv')[limit:limit+size]\n",
    "    except:\n",
    "        sys.exit('chunk too small')\n",
    "\n",
    "    glitches = []\n",
    "\n",
    "    bad_counter = 0\n",
    "    with mp.Pool(processes=64) as pool:\n",
    "        with tqdm(total=len(H1_O3a['ifo'])) as pbar:\n",
    "            pbar.set_description(\"MP-Glitch Retrieval GWOSC-GWSPY\")\n",
    "            for glitch in pool.imap_unordered(get_glitch_data, [(idx, H1_O3a) for idx in range(limit, limit+size)]):\n",
    "                if isinstance(glitch, np.ndarray):\n",
    "                    glitches.append(glitch)\n",
    "                else:\n",
    "                    bad_counter+=1\n",
    "                pbar.update()\n",
    "\n",
    "                if sanity_plot:\n",
    "                    raise NotImplemented('Return values of function incomplete!')\n",
    "                    if isinstance(glitch, np.ndarray):\n",
    "                        glitches.append(glitch)\n",
    "                        plt.figure(figsize=(16.0, 6.0), dpi=300)\n",
    "                        plt.plot(to, oglitch/max(oglitch), label='original')\n",
    "                        plt.plot(td, glitch/max(glitch), label='downsampled')\n",
    "                        plt.legend()\n",
    "                        plt.grid(which='both')\n",
    "                        plt.savefig('glitch.png')\n",
    "\n",
    "                        plt.figure(figsize=(16.0, 6.0), dpi=300)\n",
    "                        f, t, Sxx = signal.spectrogram(glitch, fs=2048.)\n",
    "                        plt.pcolormesh(t, f, Sxx, shading='gouraud')\n",
    "                        plt.ylabel('Frequency [Hz]')\n",
    "                        plt.xlabel('Time [sec]')\n",
    "                        plt.savefig('spectrogram_downsampled_glitch.png')\n",
    "\n",
    "                        plt.figure(figsize=(16.0, 6.0), dpi=300)\n",
    "                        f, t, Sxx = signal.spectrogram(oglitch, fs=4096.)\n",
    "                        plt.pcolormesh(t, f, Sxx, shading='gouraud')\n",
    "                        plt.ylabel('Frequency [Hz]')\n",
    "                        plt.xlabel('Time [sec]')\n",
    "                        plt.savefig('spectrogram_original_glitch.png')\n",
    "                        raise\n",
    "\n",
    "            print('Bad samples (not collected) = {}'.format(bad_counter))\n",
    "            glitches = np.array(glitches).astype(np.float32)\n",
    "            with h5py.File('./glitch_L1_O3b_chunk_{}.hdf'.format(chunk_num), 'a') as hf:\n",
    "                hf.create_dataset('data', data=glitches, compression=\"gzip\", chunks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if saved glitches file has valid data\n",
    "\"\"\"\n",
    "lookup_table = \"./glitch_H1_O3a_chunk_2.hdf\"\n",
    "with h5py.File(lookup_table, 'a') as fp:\n",
    "    glitch_data = np.array(fp['data'][:])\n",
    "    print(glitch_data.shape)\n",
    "raise\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing down a bash code to run the above fetcher\n",
    "\"\"\"\n",
    "#!/bin/bash\n",
    "counter=1\n",
    "batch_size=1000\n",
    "for limit in {0..80000..1000}; do\n",
    "    python3 check.py $counter $limit $batch_size\n",
    "    ((counter++))\n",
    "done;\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all file lengths for quick loading during OTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1_O3b\n",
      "L1_O3b\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "H1_O3a_dirname=\"/local/scratch/igr/nnarenraju/gwspy/H1_O3a_glitches\" \n",
    "L1_O3a_dirname=\"/local/scratch/igr/nnarenraju/gwspy/L1_O3a_glitches\"\n",
    "H1_O3b_dirname=\"/local/scratch/igr/nnarenraju/gwspy/H1_O3b_glitches\" \n",
    "L1_O3b_dirname=\"/local/scratch/igr/nnarenraju/gwspy/L1_O3b_glitches\"\n",
    "   \n",
    "# Glitch data files\n",
    "\"\"\"\n",
    "print('H1_O3a')\n",
    "glitch_files_H1_O3a = [h5py.File(fname) for fname in glob.glob(os.path.join(H1_O3a_dirname, \"*.hdf\"))]\n",
    "num_glitches_H1_O3a = [len(np.array(hf['data'][:])) for hf in glitch_files_H1_O3a]\n",
    "np.save(\"./tmp/h1_o3a.npy\", num_glitches_H1_O3a)\n",
    "\n",
    "print('L1_O3a')\n",
    "glitch_files_L1_O3a = [h5py.File(fname) for fname in glob.glob(os.path.join(L1_O3a_dirname, \"*.hdf\"))]\n",
    "num_glitches_L1_O3a = [len(np.array(hf['data'][:])) for hf in glitch_files_L1_O3a]\n",
    "np.save(\"./tmp/l1_o3a.npy\", num_glitches_L1_O3a)\n",
    "\"\"\"\n",
    "print('H1_O3b')\n",
    "glitch_files_H1_O3b = [h5py.File(fname) for fname in glob.glob(os.path.join(H1_O3b_dirname, \"*.hdf\"))]\n",
    "num_glitches_H1_O3b = [len(np.array(hf['data'][:])) for hf in glitch_files_H1_O3b]\n",
    "np.save(\"./tmp/H1_O3b.npy\", num_glitches_H1_O3b)\n",
    "\n",
    "print('L1_O3b')\n",
    "glitch_files_L1_O3b = [h5py.File(fname) for fname in glob.glob(os.path.join(L1_O3b_dirname, \"*.hdf\"))]\n",
    "num_glitches_L1_O3b = [len(np.array(hf['data'][:])) for hf in glitch_files_L1_O3b]\n",
    "np.save(\"./tmp/L1_O3b.npy\", num_glitches_L1_O3b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Noise Slice from n-days of noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The following can be directly integrated into transforms.py in ORChiD\n",
    "# Use either method 1 or method 2 for loading durations\n",
    "# NOTE: Method 1 seems to be slow sometimes\n",
    "class RandomNoiseSlice():\n",
    "    \"\"\" Used to augment the start time of noise samples from continuous noise .hdf file \"\"\"\n",
    "    # This will become the primary noise reading function\n",
    "    def __init__(self, real_noise_path=\"\", sample_length=17.0,\n",
    "                 segment_llimit=None, segment_ulimit=None):\n",
    "        self.sample_length = sample_length\n",
    "        self.min_segment_duration = self.sample_length\n",
    "        self.real_noise_path = real_noise_path\n",
    "        self.segment_ends_buffer = 0.0 # seconds\n",
    "        self.slide_buffer = 240.0\n",
    "        self.dt = 1./2048.\n",
    "\n",
    "        # Keep all required noise files open\n",
    "        self.O3a_real_noise = h5py.File(self.real_noise_path, 'r')\n",
    "        # Get detectors used\n",
    "        self.detectors = ['H1', 'L1']\n",
    "        # Get ligo segments and load_times from noise file\n",
    "        ligo_segments, load_times = self._get_ligo_segments()\n",
    "        # Get segment info and set probability of obtaining sample from segment\n",
    "        self.psegment = {}\n",
    "        segdurs = np.empty(len(ligo_segments), dtype=np.float64)\n",
    "        # limit check\n",
    "        if segment_ulimit == -1:\n",
    "            segment_ulimit = len(ligo_segments)\n",
    "\n",
    "        ### METHOD 1: Get signal durations before running pipeline ###\n",
    "        psegment = {}\n",
    "        for n, seg in enumerate(ligo_segments):\n",
    "            key_time = str(load_times[seg][0])\n",
    "            _key = f'{self.detectors[0]}/{key_time}'\n",
    "            # Sanity check if _key is present in noise file\n",
    "            try:\n",
    "                _ = self.O3a_real_noise[_key]\n",
    "            except:\n",
    "                # An impossible segment duration and cond rand < segprob is never satisfied\n",
    "                segdurs[n] = 0\n",
    "                psegment[n] = [-1, -1, -1]\n",
    "                continue\n",
    "            \n",
    "            # Set valid start and end times of given segment (not actual start time)\n",
    "            # load_times[seg][0] is the same as seg[0]\n",
    "            segment_length = len(np.array(self.O3a_real_noise[_key][:]))\n",
    "            seg_start_idx = 0 + self.segment_ends_buffer\n",
    "            seg_end_idx = segment_length - (self.sample_length + self.segment_ends_buffer)*(1./self.dt)\n",
    "            # Get segment duration for calculating sampling ratio wrt all segments\n",
    "            segdurs[n] = segment_length\n",
    "            # Add the epoch parameter to store\n",
    "            psegment[n] = [key_time, seg_start_idx, seg_end_idx]\n",
    "        \n",
    "        print(self.real_noise_path)\n",
    "        \n",
    "        ### METHOD 2: Pre-saved durations ###\n",
    "        \"\"\"\n",
    "        lookup = np.load(\"./notebooks/tmp/segdurs_all.npy\")\n",
    "        for n, seg in enumerate(ligo_segments):\n",
    "            key_time = str(load_times[seg][0])\n",
    "            _key = f'{self.detectors[0]}/{key_time}'\n",
    "            # Sanity check if _key is present in noise file\n",
    "            if n >= segment_llimit and n <= segment_ulimit:\n",
    "                segment_length = lookup[:,1][n]\n",
    "                seg_start_idx = 0 + self.segment_ends_buffer\n",
    "                seg_end_idx = segment_length - (self.sample_length + self.segment_ends_buffer)*(1./self.dt)\n",
    "                segdurs[n] = lookup[:,1][n]\n",
    "                self.psegment[n] = [key_time, seg_start_idx, seg_end_idx]\n",
    "            else:\n",
    "                # An impossible segment duration and cond rand < segprob is never satisfied\n",
    "                segdurs[n] = 0\n",
    "                self.psegment[n] = [-1, -1, -1]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get probabilties of using segment using segment durations\n",
    "        seg_idx = np.arange(len(segdurs))\n",
    "        segprob = list(segdurs/np.sum(segdurs))\n",
    "        # Get one choice from seg_idx based on probalities obtained from seg durations\n",
    "        self.segment_choice = lambda _: np.random.choice(seg_idx, 1, p=segprob)[0]\n",
    "\n",
    "    def _load_segments(self):\n",
    "        tmp_dir = \"./tmp\"\n",
    "        path = os.path.join(tmp_dir, 'segments.csv')\n",
    "        # Download data if it does not exist\n",
    "        if not os.path.isfile(path):\n",
    "            url = 'https://www.atlas.aei.uni-hannover.de/work/marlin.schaefer/MDC/segments.csv'\n",
    "            response = requests.get(url)\n",
    "            with open(path, 'wb') as fp:\n",
    "                fp.write(response.content)\n",
    "\n",
    "        # Load data from CSV file\n",
    "        segs = ligo.segments.segmentlist([])\n",
    "        with open(path, 'r') as fp:\n",
    "            reader = csv.reader(fp)\n",
    "            for i, row in enumerate(reader):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                idx, start, end = row\n",
    "                segs.append(ligo.segments.segment([int(start), int(end)]))\n",
    "\n",
    "        return segs\n",
    "    \n",
    "    def _get_ligo_segments(self):\n",
    "        # https://lscsoft.docs.ligo.org/ligo-segments/\n",
    "        segments = self._load_segments()\n",
    "        \n",
    "        # Restrict segments\n",
    "        ligo_segments = ligo.segments.segmentlist([])\n",
    "        for seg in segments:\n",
    "            start, end = seg\n",
    "            segduration = end - start\n",
    "            # Check if segment fulfills minimum duration requirements\n",
    "            if self.min_segment_duration is not None and segduration - self.slide_buffer < self.min_segment_duration:\n",
    "                continue\n",
    "            ligo_segments.append(ligo.segments.segment([start, end]))\n",
    "        \n",
    "        # Refer link provided above to ligo-segments\n",
    "        # Sort the elements of the list into ascending order, and merge continuous \n",
    "        # segments into single segments. Segmentlist is modified in place. \n",
    "        # This operation is O(n log n).\n",
    "        ligo_segments.coalesce()\n",
    "\n",
    "        # Get times from each valid segment\n",
    "        load_times = {}\n",
    "        for seg in ligo_segments:\n",
    "            for rawseg in segments:\n",
    "                if seg in rawseg:\n",
    "                    load_times[seg] = rawseg\n",
    "                    break;\n",
    "            if seg not in load_times:\n",
    "                raise RuntimeError\n",
    "            \n",
    "        return ligo_segments, load_times\n",
    "    \n",
    "    def _make_sample_start_time(self, seg_start_idx, seg_end_idx):\n",
    "        # Make a sample start time that is uniformly distributed within segdur\n",
    "        return int(np.random.uniform(low=seg_start_idx, high=seg_end_idx))\n",
    "\n",
    "    def get_noise_segment(self, segdeets):\n",
    "        ## Get noise sample from given O3a real noise segment\n",
    "        noise = []\n",
    "        for det, segdeet in zip(self.detectors, segdeets):\n",
    "            key_time, seg_start_idx, seg_end_idx = segdeet\n",
    "            # Get sample_start_time using segment times\n",
    "            # This start time will lie within a valid segment time interval\n",
    "            sample_start_idx = self._make_sample_start_time(seg_start_idx, seg_end_idx)\n",
    "            # Get the required portion of given segment\n",
    "            sidx = sample_start_idx\n",
    "            eidx = sample_start_idx + int(self.sample_length / self.dt)\n",
    "            # Which key does the current segment belong to in real noise file\n",
    "            # key_time provided is the start time of required segment\n",
    "            key = f'{det}/{key_time}'\n",
    "            # Get time series from segment and apply the dynamic range factor\n",
    "            ts = np.array(self.O3a_real_noise[key][sidx:eidx]).astype(np.float64)\n",
    "            if \"O3a_real_noise.hdf\" in self.real_noise_path:\n",
    "                ts /= DYN_RANGE_FAC\n",
    "            noise.append(ts)\n",
    "        \n",
    "        # Convert noise into np.ndarray, suitable for other transformations\n",
    "        noise = np.stack(noise, axis=0)\n",
    "        return noise\n",
    "    \n",
    "    def pick_segment(self):\n",
    "        # Pick a random segment to use based on probablities set using their duration\n",
    "        # Picking two different segments and start times provides an extra layer of augmentation\n",
    "        idx1 = self.segment_choice(0)\n",
    "        idx2 = self.segment_choice(0)\n",
    "        # Return the segment details of selected segment\n",
    "        return (self.psegment[idx1], self.psegment[idx2])\n",
    "\n",
    "    def apply(self):\n",
    "        ## Get noise sample with random start time from O3a real noise\n",
    "        # Toss a biased die and retrieve the segment to use\n",
    "        segdeets = self.pick_segment()\n",
    "        # Get noise sample with random start time (uniform within segment)\n",
    "        noise = self.get_noise_segment(segdeets)\n",
    "        # Return noise data\n",
    "        return noise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Recolouring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recolour(self, ts, DET):\n",
    "    ## Whiten the noise using old PSD and recolour using new PSD\n",
    "    # Convert to frequency domain after windowing\n",
    "    n = len(ts)\n",
    "    delta_t = 1./self.fs\n",
    "    data_fd = np.fft.rfft(ts)\n",
    "    freq = np.fft.rfftfreq(n, delta_t)\n",
    "    data_delta_f = freq[1] - freq[0]\n",
    "    # Convert the PSD to new delta_f using PyCBC interpolate function\n",
    "    old_psd = FrequencySeries(DET['old_psd'], delta_f=DET['old_delta_f'])\n",
    "    old_psd = interpolate(old_psd, data_delta_f)\n",
    "    # Whitening (Remove old PSD from data)\n",
    "    whitened_signal = data_fd / np.sqrt(old_psd)\n",
    "    # Convert the new PSDs to have delta_f similar to data\n",
    "    new_psd = FrequencySeries(DET['new_psd'], delta_f=DET['old_delta_f'])\n",
    "    new_psd = interpolate(new_psd, data_delta_f)\n",
    "    # Recolour using new PSD and return to time domain\n",
    "    recoloured = np.fft.irfft(whitened_signal*np.sqrt(new_psd))\n",
    "    tmp = TimeSeries(recoloured, delta_t=1./self.fs)\n",
    "    recoloured = pycbc_highpass(tmp, self.noise_low_freq_cutoff).astype(np.float64)\n",
    "    # debug plotter\n",
    "    if self.debug_me:\n",
    "        _, recovered = scipy_welch(recoloured, fs=self.fs, nperseg=4.*self.fs, average='median')\n",
    "        self.debug_recolour([old_psd, new_psd, ts, whitened_signal, recoloured, recovered],\n",
    "                            ['old_psd', 'new_psd', 'original', 'white', 'recoloured', 'recovered_psd'])\n",
    "\n",
    "    return recoloured.numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

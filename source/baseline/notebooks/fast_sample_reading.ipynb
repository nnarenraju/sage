{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we are using non-OTF data reading methods\n",
    "# WARNING!!! This is not a fully working code. This is skeletal toy model that needs to be \n",
    "# altered based on requirement.\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "class ReadContiguousData():\n",
    "    \"\"\" Used to augment the start time of noise samples from continuous noise .hdf file \"\"\"\n",
    "    # This will become the primary noise reading function\n",
    "    def __init__(self, data_path=\"\", sample_length=17.0):\n",
    "        self.sample_length = sample_length\n",
    "        self.data_path = data_path\n",
    "        self.dt = None\n",
    "        self.detectors = None\n",
    "        # Keep all required files open\n",
    "        self.data = h5py.File(self.data_path, 'r')\n",
    "    \n",
    "    def _make_sample_start_time(self, start_idx, end_idx):\n",
    "        # Make a sample start time that is uniformly distributed within segdur\n",
    "        return int(np.random.uniform(low=start_idx, high=end_idx))\n",
    "\n",
    "    def get_data(self, idx_params):\n",
    "        sample = []\n",
    "        for det, idx_param in zip(self.detectors, idx_params):\n",
    "            key, start_idx, end_idx = idx_param\n",
    "            sample_start_idx = self._make_sample_start_time(start_idx, end_idx)\n",
    "            # Get the required portion of given segment\n",
    "            sidx = sample_start_idx\n",
    "            eidx = sample_start_idx + int(self.sample_length / self.dt)\n",
    "            key = f'{det}/{key}' # address of hdf5 dataset required\n",
    "            # Get time series from segment\n",
    "            ts = np.array(self.data[key][sidx:eidx]).astype(np.float64)\n",
    "            sample.append(ts)\n",
    "\n",
    "        noise = np.stack(noise, axis=0)\n",
    "        return noise\n",
    "    \n",
    "    def get_idx_params(self):\n",
    "        # Write a function to get start and end idx using segment duration\n",
    "        # This can be called and saved in __init__ if needed\n",
    "        pass\n",
    "\n",
    "    def apply(self):\n",
    "        idx_params = self.get_idx_params()\n",
    "        # Get sample with random start time (uniform within segment)\n",
    "        noise = self.get_noise_segment(idx_params)\n",
    "        # Return noise data\n",
    "        return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using ExternalLinks to access multiple files without opening all of them in an array\n",
    "import os\n",
    "import glob\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "def make_elink_lookup(self):\n",
    "    # Make a lookup table with (id, path, target) for each training data\n",
    "    # Should have an equal ratio, shuffled, of both classes (signal and noise)\n",
    "    # Save the dataset paths alongside the target and ids as hdf5\n",
    "    \n",
    "    # Other params to be saved in extlink file\n",
    "    other_params = ['gw_param1', 'gw_param2']\n",
    "    # Save params\n",
    "    all_data = {'id': [], 'path': [], 'target': []}\n",
    "    # Save other params into all_data\n",
    "    oparams_dict = {foo: [] for foo in other_params}\n",
    "    all_data.update(oparams_dict)\n",
    "    \n",
    "    save_dir = \"\"\n",
    "    lookup = os.path.join(save_dir, 'extlinks.hdf')\n",
    "    main = h5py.File(lookup, 'a', libver='latest')\n",
    "    \n",
    "    idx = 0\n",
    "\n",
    "    # tmp var\n",
    "    tmp_var = {foo:[] for foo in all_data.keys()}\n",
    "    # Add attributes if present\n",
    "    add_attrs = True\n",
    "    \n",
    "    dataset_dir = \"\"\n",
    "    chunks = os.path.join(dataset_dir, \"*.hdf\")\n",
    "    for nfile, chunk_file in enumerate(glob.glob(chunks)):\n",
    "        ## Read the file, get all data and add data as ExternalLink to extlinks.hdf file\n",
    "        # Read HDF5 tree and add tree to ExternalLink\n",
    "        with h5py.File(chunk_file, 'r') as h5f:\n",
    "            # fp.keys() --> Groups in HDF5 file\n",
    "            # fp[group_key].keys() --> Further groups present within\n",
    "            grps = list(h5f.keys())\n",
    "            \n",
    "            for grp in grps:\n",
    "                # Get sample length of each dataset\n",
    "                # This will make a branch with chunk file number in the address\n",
    "                # Like: 007/grp (007 being the file number)\n",
    "                _branch = f\"{nfile:03}\" + '/' + grp\n",
    "                \n",
    "                for obj in h5f[grp].keys():\n",
    "                    # Add dataset as External Link\n",
    "                    # Branch Format: 007 / group / sub_group / dataset\n",
    "                    curr_branch = grp + '/' + obj # branch in chunk file to refer\n",
    "                    extl_branch = _branch + '/' + obj # branch details for external link\n",
    "                    # Make external link\n",
    "                    main[extl_branch] = h5py.ExternalLink(chunk_file, curr_branch)\n",
    "                \n",
    "                # Maxshape of given np.ndarray\n",
    "                shape = np.array(h5f[grp][list(h5f[grp].keys())[0]]).shape[0]\n",
    "                # Other params of each sample\n",
    "                for oparam in other_params:\n",
    "                    # Assuming other datasets present (like gw params)\n",
    "                    if oparam in h5f[grp].keys():\n",
    "                        oparam_ = np.array(h5f[grp][oparam])\n",
    "                    else:\n",
    "                        # Noise files may not have these params\n",
    "                        # So we can save a bunch of -1's in them for constant maxshape\n",
    "                        oparam_ = np.full(shape, -1)\n",
    "                    # Update oparam\n",
    "                    tmp_var[oparam].extend(oparam_)\n",
    "\n",
    "                # Update idxs (unique ids for each sample)\n",
    "                branch_ids = np.arange(idx, idx+shape)\n",
    "                tmp_var['id'].extend(branch_ids)\n",
    "                idx = idx+shape\n",
    "                # Update paths to dataset objects (address to each unique sample)\n",
    "                # We external linked entire array to extlinks.hdf\n",
    "                # We need to give unique addresses to each sample in that nd-array\n",
    "                # Here shape will be the number of samples we have in said array\n",
    "                branch_paths = itertools.product([_branch], np.arange(shape))\n",
    "                tmp_var['path'].extend([foo + '/' + str(bar) for foo, bar in branch_paths])\n",
    "                # Update target variable for each sample\n",
    "                target = None\n",
    "                tmp_var['target'].extend(np.full(shape, target))\n",
    "                    \n",
    "            # Add attributes from chunk files once to main ExternalLink File\n",
    "            if add_attrs:\n",
    "                attrs_save = dict(h5f.attrs)\n",
    "                for key, value in attrs_save.items():\n",
    "                    main.attrs[key] = value\n",
    "                add_attrs = False\n",
    "\n",
    "    # Sanity check\n",
    "    # Uncomment this to check whether external links worked properly\n",
    "    \"\"\" \n",
    "    with h5py.File(self.dirs['lookup'], 'a', libver='latest') as fp:\n",
    "        start = time.time()\n",
    "        # Change address to known address\n",
    "        # This is an example from the ORChiD pipeline\n",
    "        print(np.array(fp['004/2048/signal/h_plus'][2]))\n",
    "        fin = time.time() - start\n",
    "        print(fin)\n",
    "    raise\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Create lookup using zip (use column_stack instead)\n",
    "    # Explicitly check for length inconsistancies. zip doesn't raise error.\n",
    "    tmp_var_lens = [len(tmp_var[foo]) for foo in tmp_var.keys()]\n",
    "    # Checks if all lengths are the same (rewrite this in a better way, using numpy probably)\n",
    "    assert len(list(set(tmp_var_lens))) == 1, 'var:tmp_var fields in extlinks have inconsistent column lengths!'\n",
    "    \n",
    "    lookup = list(zip(*tmp_var.values()))\n",
    "    # Shuffle the column stack (signal and noise are not shuffled)\n",
    "    random.shuffle(lookup)\n",
    "    # Separate out the tuples for ids, paths and targets\n",
    "    for foo, key in zip(zip(*lookup), tmp_var.keys()):\n",
    "        all_data[key].extend(foo)\n",
    "    \n",
    "    # Close file explicitly, or use with instead\n",
    "    main.close()\n",
    "    \n",
    "    # Write required fields as datasets in HDF5 extlinks.hdf file\n",
    "    with h5py.File(lookup, 'a') as ds:\n",
    "        \"\"\"\n",
    "        Shuffle Filter for HDF5:\n",
    "            Block-oriented compressors like GZIP or LZF work better when presented with \n",
    "            runs of similar values. Enabling the shuffle filter rearranges the bytes in \n",
    "            the chunk and may improve compression ratio. No significant speed penalty, \n",
    "            lossless.\n",
    "        \"\"\"\n",
    "        for _param in all_data.keys():\n",
    "            ds.create_dataset(_param, data=all_data[_param], compression='gzip', compression_opts=9, shuffle=True)\n",
    "        \n",
    "    print(\"make_elink_lookup: ExternalLink lookup table created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the external links are created, we can access them using the unique addresses stored in the extlinks file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_extlinks():\n",
    "    # Set lookup table ('training.hdf' lookup table is also present for batch-loading)\n",
    "    lookup_file = 'extlinks.hdf'\n",
    "\n",
    "    export_dir = \"\"\n",
    "    lookup_table = os.path.join(export_dir, lookup_file)\n",
    "    # Deprecated: Usage of self.get_metadata with make_default_dataset (non-MP)\n",
    "    with h5py.File(lookup_table, 'a') as fp:\n",
    "        ids = np.array(fp['id'][:])\n",
    "        # For reading strings\n",
    "        paths = np.array([foo.decode('utf-8') for foo in fp['path']])\n",
    "        targets = np.array(fp['target'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here data_path is a single instance of paths (addresses to unique samples)\n",
    "# In a class, open the extliks file as self.extmain\n",
    "# We can then use the addresses to access the linked samples\n",
    "\n",
    "def read(self, data_path):\n",
    "    # Store all params within chunk file\n",
    "    params = {}\n",
    "    targets = {}\n",
    "    \n",
    "    # Get data from ExternalLink'ed lookup file\n",
    "    # The addresses contain dataset address and sample idx\n",
    "    # We split them to we access the address normally and access the required idx\n",
    "    HDF5_Dataset, didx = os.path.split(data_path)\n",
    "    # Dataset Index should be an integer\n",
    "    didx = int(didx)\n",
    "    # Access group\n",
    "    group = self.extmain[HDF5_Dataset]\n",
    "    \n",
    "    # Targets can be stored in the extlinks file\n",
    "    target = None\n",
    "    targets['gw'] = target\n",
    "\n",
    "    if not target:\n",
    "        ## Read noise data (for two detectors, each stored as separate dataset)\n",
    "        noise_1 = np.array(group['noise_1'][didx])\n",
    "        noise_2 = np.array(group['noise_2'][didx])\n",
    "        sample = np.stack([noise_1, noise_2], axis=0)\n",
    "    else:\n",
    "        ## Read signal data\n",
    "        h_plus = np.array(group['h_plus'][didx])\n",
    "        h_cross = np.array(group['h_cross'][didx])\n",
    "        sample = np.stack([h_plus, h_cross], axis=0)\n",
    "        # Signal params\n",
    "        params['mass1'] = group['mass1'][didx]\n",
    "        params['mass2'] = group['mass2'][didx]\n",
    "        params['distance'] = group['distance'][didx]\n",
    "        params['mchirp'] = group['mchirp'][didx]\n",
    "        params['tc'] = group['tc'][didx]\n",
    "        params['ra'] = group['ra'][didx]\n",
    "        params['dec'] = group['dec'][didx]\n",
    "        params['polarization'] = group['polarization'][didx]\n",
    "    \n",
    "    # Generic params\n",
    "    params['sample_rate'] = self.sample_rate\n",
    "    params['noise_low_freq_cutoff'] = self.noise_low_freq_cutoff\n",
    "    \n",
    "    return (sample, targets, params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

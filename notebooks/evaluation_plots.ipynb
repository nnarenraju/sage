{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \"Wswiglal-redir-stdio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "import corner\n",
    "import scipy\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "import matplotlib as mpl\n",
    "from functools import partial\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "from evaluator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access results file for D4 testing\n",
    "# parent_dir = \"/local/scratch/igr/nnarenraju/testing_month_D4_seeded/\"\n",
    "parent_dir = \"/home/nnarenraju/Research/ORChiD/test_data_d3/\"\n",
    "foreground = os.path.join(parent_dir, 'foreground.hdf')\n",
    "background = os.path.join(parent_dir, 'background.hdf')\n",
    "## BEST June\n",
    "# foutput = os.path.join(parent_dir, 'testing_foutput_BEST_June.hdf')\n",
    "# boutput = os.path.join(parent_dir, 'testing_boutput_BEST_June.hdf')\n",
    "## BEST Feb\n",
    "# foutput = os.path.join(parent_dir, 'testing_foutput_BEST_Feb.hdf')\n",
    "# boutput = os.path.join(parent_dir, 'testing_boutput_BEST_Feb.hdf')\n",
    "## Kennebec Annealed\n",
    "# foutput = os.path.join(parent_dir, 'testing_foutput_annealed_training.hdf')\n",
    "# boutput = os.path.join(parent_dir, 'testing_boutput_annealed_training.hdf')\n",
    "## Metric latest?\n",
    "# foutput = os.path.join(parent_dir, 'testing_foutput_metric_latest.hdf')\n",
    "# boutput = os.path.join(parent_dir, 'testing_boutput_metric_latest.hdf')\n",
    "## D3 Sage\n",
    "# foutput = os.path.join(parent_dir, 'testing_foutput_D3_SageNet.hdf')\n",
    "# boutput = os.path.join(parent_dir, 'testing_boutput_D3_SageNet.hdf')\n",
    "## uTau noPE\n",
    "# foutput = \"/local/scratch/igr/nnarenraju/testing_month_D4_seeded/uTau_unbounded_noPE/testing_foutput.hdf\"\n",
    "# boutput = \"/local/scratch/igr/nnarenraju/testing_month_D4_seeded/uTau_unbounded_noPE/testing_boutput.hdf\"\n",
    "\n",
    "injections = os.path.join(parent_dir, 'injections.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "far_scaling_factor = 2592000.0 # 30 days in seconds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices contained in foreground\n",
    "print(\"Finding injections contained in data\")\n",
    "padding_start, padding_end = 30, 30\n",
    "dur, idxs = find_injection_times([foreground],\n",
    "                                 injections,\n",
    "                                 padding_start=padding_start,\n",
    "                                 padding_end=padding_end)\n",
    "if np.sum(idxs) == 0:\n",
    "    msg = 'The foreground data contains no injections! '\n",
    "    msg += 'Probably a too small section of data was generated. '\n",
    "    msg += 'Please make sure to generate at least {} seconds of data. '\n",
    "    msg += 'Otherwise a sensitive distance cannot be calculated.'\n",
    "    msg = msg.format(padding_start + padding_end + 24)\n",
    "    raise RuntimeError(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get injections optimal SNRs\n",
    "_parent_dir = \"/local/scratch/igr/nnarenraju/testing_month_D4_seeded/\"\n",
    "snrs_path = os.path.join(_parent_dir, \"snr.hdf\")\n",
    "if os.path.exists(snrs_path):\n",
    "    with h5py.File(snrs_path, 'r') as fp:\n",
    "        snrs = fp['snr'][()]\n",
    "\n",
    "snrs = np.array(snrs)\n",
    "snrs = snrs[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snrs = np.full(len(idxs), 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This checks out\n",
    "print('Duration calculated by find_injection_times = {}'.format(dur))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 3\n",
    "\n",
    "team_1 = {'name': 'Sage'}\n",
    "team_2 = {'name': 'PyCBC'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injparams = {}\n",
    "with h5py.File(injections, 'r') as fp:\n",
    "    params = list(fp.keys())\n",
    "    for param in params:\n",
    "        data = fp[param][()]\n",
    "        injparams[param] = data[idxs]\n",
    "        \n",
    "    use_chirp_distance = 'chirp_distance' in params\n",
    "\n",
    "other_results = \"/home/nnarenraju/Research/ORChiD/results\"\n",
    "other_teams = os.listdir(other_results)\n",
    "\n",
    "print('Dataset {} comparing {} against {}'.format(dataset, team_1['name'], team_2['name']))\n",
    "team_1['fgpath'] = foutput\n",
    "team_1['bgpath'] = boutput\n",
    "    \n",
    "if team_2['name'] == 'PyCBC':\n",
    "    team_2['fgpath'] = os.path.join(other_results, \"{}/ds{}/fg.hdf\".format(team_2['name'], dataset))\n",
    "    team_2['bgpath'] = os.path.join(other_results, \"{}/ds{}/bg.hdf\".format(team_2['name'], dataset))\n",
    "if team_2['name'] == 'aresgw':\n",
    "    team_2['fgpath'] = \"/home/nnarenraju/Research/ORChiD/gw-detection-deep-learning/results_best/fg.hdf\"\n",
    "    team_2['bgpath'] = \"/home/nnarenraju/Research/ORChiD/gw-detection-deep-learning/results_best/bg.hdf\"\n",
    "\n",
    "\n",
    "for nteam in [1, 2]:\n",
    "    team = locals()[\"team_{}\".format(nteam)]\n",
    "    print('\\nTeam {}'.format(team))\n",
    "    # Read foreground events\n",
    "    print(f'Reading foreground events from {team[\"fgpath\"]}')\n",
    "    fg_events = []\n",
    "    with h5py.File(team['fgpath'], 'r') as fp:\n",
    "        fg_events.append(np.vstack([fp['time'], fp['stat'], np.array(fp['var'])]))\n",
    "    team['fgevents'] = np.concatenate(fg_events, axis=-1)\n",
    "    \n",
    "    # Read background events\n",
    "    print(f'Reading background events from {team[\"bgpath\"]}')\n",
    "    bg_events = []\n",
    "    with h5py.File(team['bgpath'], 'r') as fp:\n",
    "        bg_events.append(np.vstack([fp['time'], fp['stat'], np.array(fp['var'])]))\n",
    "    team['bgevents'] = np.concatenate(bg_events, axis=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Calculate the false-alarm rate and sensitivity of a search algorithm. \"\"\"\n",
    "\n",
    "# Get data from fg and bg events file\n",
    "print('Team 1: {}'.format(team_1['name']))\n",
    "print('Team 2: {}'.format(team_2['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add SNRs into the injparams (this will automagically include it within most plots)\n",
    "chirp_distance = use_chirp_distance\n",
    "injparams['snr'] = snrs\n",
    "output_dir = \"./evaluation_plots/\"\n",
    "# Return data tmp var\n",
    "ret = {}\n",
    "\n",
    "## COMMON ##\n",
    "# Get injection params\n",
    "injtimes = injparams['tc']\n",
    "dist = injparams['distance']\n",
    "\n",
    "# Get chirp mass from the source masses\n",
    "if chirp_distance:\n",
    "    massc = mchirp(injparams['mass1'], injparams['mass2'])\n",
    "# Set duration if nothing is passed\n",
    "duration = dur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_output(found_injections, noise_stats, output_dir, team_name, lower_threshold=0.0):\n",
    "    # Plotting the noise and signals stats for found samples\n",
    "    plt.figure(figsize=(12.0, 12.0))\n",
    "    foo = found_injections[1][found_injections[1] > lower_threshold]\n",
    "    plt.hist(foo, label='found_injections', bins=100, alpha=0.8)\n",
    "    noise_stats = noise_stats[noise_stats > lower_threshold]\n",
    "    plt.hist(noise_stats, label='noise', bins=100, alpha=0.8)\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, which='both')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_dir, 'network_output_{}.png'.format(team_name)))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for team in [team_1, team_2]:\n",
    "    \n",
    "    print('\\nTeam {}'.format(team['name']))\n",
    "    print('Sorting foreground event times')\n",
    "    sidxs = team[\"fgevents\"][0].argsort()\n",
    "    fgevents = team[\"fgevents\"].T[sidxs].T\n",
    "    \n",
    "    logging.info('Finding injection times closest to event times')\n",
    "    idxs = find_closest_index(injtimes, fgevents[0])\n",
    "    diff = np.abs(injtimes[idxs] - fgevents[0])\n",
    "    \n",
    "    # If the difference between the injection time and trigger is within tc variance\n",
    "    # The trigger is identified as an event (there may be duplicate triggers)\n",
    "    logging.info('Finding true- and false-positives')\n",
    "    tpbidxs = diff <= fgevents[2]\n",
    "    tpidxs = np.arange(len(fgevents[0]))[tpbidxs]\n",
    "    fpbidxs = diff > fgevents[2]\n",
    "    fpidxs = np.arange(len(fgevents[0]))[fpbidxs]\n",
    "    \n",
    "    tpevents = fgevents.T[tpidxs].T\n",
    "    fpevents = fgevents.T[fpidxs].T\n",
    "    \n",
    "    ## Update the returns dictionary\n",
    "    if team['name'] == \"Sage\":\n",
    "        ret['fg-events'] = fgevents\n",
    "        ret['found-indices'] = np.arange(len(injtimes))[idxs]\n",
    "        ret['missed-indices'] = np.setdiff1d(np.arange(len(injtimes)), ret['found-indices'])\n",
    "        ret['true-positive-event-indices'] = tpidxs\n",
    "        ret['false-positive-event-indices'] = fpidxs\n",
    "        ret['sorting-indices'] = sidxs\n",
    "        ret['true-positive-diffs'] = diff[tpidxs]\n",
    "        ret['false-positive-diffs'] = diff[fpidxs]\n",
    "        ret['true-positives'] = tpevents\n",
    "        ret['false-positives'] = fpevents\n",
    "    \n",
    "    # Calculate foreground FAR\n",
    "    logging.info('Calculating foreground FAR')\n",
    "    noise_stats_fg = fpevents[1].copy()\n",
    "    noise_stats_fg.sort()\n",
    "    fgfar = len(noise_stats_fg) - np.arange(len(noise_stats_fg)) - 1\n",
    "    fgfar = fgfar / duration\n",
    "    if team['name'] == \"Sage\":\n",
    "        ret['fg-far'] = fgfar\n",
    "    \n",
    "    # Calculate background FAR\n",
    "    logging.info('Calculating background FAR')\n",
    "    noise_stats = team[\"bgevents\"][1].copy()\n",
    "    noise_stats.sort()\n",
    "    far = len(noise_stats) - np.arange(len(noise_stats)) - 1\n",
    "    far = far / duration\n",
    "    if team['name'] == \"Sage\":\n",
    "        ret['far'] = far\n",
    "    \n",
    "    # Find best true-positive for each injection\n",
    "    found_injections = []\n",
    "    tmpsidxs = idxs.argsort()\n",
    "    sorted_idxs = idxs[tmpsidxs]\n",
    "    iidxs = np.full(len(idxs), False)\n",
    "    for i in tqdm(range(len(injtimes)), ascii=True, desc='Determining found injections'):\n",
    "        L = np.searchsorted(sorted_idxs, i, side='left')\n",
    "        if L >= len(idxs) or sorted_idxs[L] != i:\n",
    "            continue\n",
    "        R = np.searchsorted(sorted_idxs, i, side='right')\n",
    "        # All indices that point to the same injection\n",
    "        iidxs[tmpsidxs[L:R]] = True\n",
    "        # Indices of the true-positives that belong to the same injection\n",
    "        eidxs = np.logical_and(iidxs[tmpsidxs[L:R]],\n",
    "                                tpbidxs[tmpsidxs[L:R]])\n",
    "        if eidxs.any():\n",
    "            found_injections.append([i, np.max(fgevents[1][tmpsidxs[L:R]][eidxs])])\n",
    "        iidxs[tmpsidxs[L:R]] = False\n",
    "\n",
    "    # Number of injections found within given testing data\n",
    "    found_injections = np.array(found_injections).T\n",
    "    print('Number of found injections = {}'.format(len(found_injections[0])))\n",
    "    \n",
    "    # Calculate sensitivity\n",
    "    # CARE! THIS APPLIES ONLY WHEN THE DISTRIBUTION IS CHOSEN CORRECTLY\n",
    "    logging.info('Calculating sensitivity')\n",
    "    sidxs = found_injections[1].argsort() # Sort found injections\n",
    "    found_injections = found_injections.T[sidxs].T\n",
    "    \n",
    "    if chirp_distance:\n",
    "        found_mchirp_total = massc[found_injections[0].astype(int)]\n",
    "        mchirp_max = massc.max()\n",
    "        \n",
    "    max_distance = dist.max()\n",
    "    # print('Maximum distance given by injections = {}'.format(max_distance))\n",
    "    vtot = (4. / 3.) * np.pi * max_distance**3.\n",
    "    Ninj = len(dist)\n",
    "    print('Total number of injections = {}'.format(Ninj))\n",
    "    \n",
    "    # Params to calculate the sensitive volume\n",
    "    if chirp_distance:\n",
    "        mc_norm = mchirp_max ** (5. / 2.) * len(massc)\n",
    "    else:\n",
    "        mc_norm = Ninj\n",
    "        \n",
    "    prefactor = vtot / mc_norm\n",
    "    nfound = len(found_injections[1]) - np.searchsorted(found_injections[1],\n",
    "                                                        noise_stats,\n",
    "                                                        side='right')\n",
    "    \n",
    "    \n",
    "    if chirp_distance:\n",
    "        # Get found chirp-mass indices for given threshold\n",
    "        fidxs = np.searchsorted(found_injections[1], noise_stats, side='right')\n",
    "        # Plotting the network output\n",
    "        network_output(found_injections, noise_stats, output_dir, team['name'], lower_threshold=-999)\n",
    "        \n",
    "        found_mchirp_total = np.flip(found_mchirp_total)\n",
    "        \n",
    "        # Calculate sum(found_mchirp ** (5/2))\n",
    "        # with found_mchirp = found_mchirp_total[i:]\n",
    "        # and i looped over fidxs\n",
    "        # Code below is a vectorized form of that\n",
    "        cumsum = np.flip(np.cumsum(found_mchirp_total ** (5./2.)))\n",
    "        cumsum = np.concatenate([cumsum, np.zeros(1)])\n",
    "        mc_sum = cumsum[fidxs]\n",
    "        Ninj = np.sum((mchirp_max / massc) ** (5. / 2.))\n",
    "        \n",
    "        cumsumsq = np.flip(np.cumsum(found_mchirp_total ** 5))\n",
    "        cumsumsq = np.concatenate([cumsumsq, np.zeros(1)])\n",
    "        sample_variance_prefactor = cumsumsq[fidxs]\n",
    "        sample_variance = sample_variance_prefactor / Ninj\\\n",
    "                            - (mc_sum / Ninj) ** 2  # noqa: E127\n",
    "    else:\n",
    "        mc_sum = nfound\n",
    "        sample_variance = nfound / Ninj - (nfound / Ninj) ** 2\n",
    "        \n",
    "    vol = prefactor * mc_sum\n",
    "    vol_err = prefactor * (Ninj * sample_variance) ** 0.5\n",
    "    rad = (3 * vol / (4 * np.pi))**(1. / 3.)\n",
    "    print('Radius or sensitive distance as calculated from the volume obtained ({})'.format(team['name']))\n",
    "    print('min rad = {}, max rad = {}'.format(min(rad), max(rad)))\n",
    "    \n",
    "    if team['name'] == \"Sage\":\n",
    "        ret['sensitive-volume'] = vol\n",
    "        ret['sensitive-distance'] = rad\n",
    "        ret['sensitive-volume-error'] = vol_err\n",
    "        ret['sensitive-fraction'] = nfound / Ninj\n",
    "    \n",
    "    if team['name'] == \"PyCBC\" or team['name'] == \"aresgw\":\n",
    "        ret['sensitive-distance-pycbc'] = rad\n",
    "        ret['far-pycbc'] = far\n",
    "    \n",
    "    # Update plotting params for each group\n",
    "    team['found_idx'] = found_injections[0].astype(int)\n",
    "    team['found_stats'] = found_injections[1]\n",
    "    # Add all found injparams to to plotting dict\n",
    "    team['params'] = list(injparams.keys())\n",
    "    team.update(injparams)\n",
    "    # The values given are indices and have to be 1 less than the number of FA per month req.\n",
    "    team['far_thresholds'] = noise_stats[::-1][[0, 3, 29, 99, 999]]\n",
    "    team['noise_stats'] = noise_stats[::-1]\n",
    "    if team['name'] == 'Sage' and False:\n",
    "        with open('noise_stats.pickle', 'wb') as handle:\n",
    "            pickle.dump(team['noise_stats'], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    team['sens_dist'] = rad\n",
    "    team['sens_frac'] = nfound / Ninj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_1['far_thresholds']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Found/Missed Corner Plot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 16,\n",
    "    \"text.latex.preamble\": r'\\usepackage{amsmath}'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colour table\n",
    "# orange/purple - \"#EB6123\", \"#512888\"\n",
    "# blue/red - \"#2F67B1\", \"#BF2C23\"\n",
    "# pale blue - \"#3A93C3\", dark blue - \"#1065AB\"\n",
    "# pale red - \"#D75F4C\", dark red - \"#B31529\"\n",
    "# pale green - \"#5CAE63\", dark green - \"#1B7939\"\n",
    "# Super light blue/green - \"#D1E5F0\", \"#D9F1D5\"\n",
    "# yellow/blue - \"#FDB338\", \"#025196\"\n",
    "# dark purple - \"#742881\"\n",
    "\n",
    "# Ross color scale - [\"#000000\",\"#6db6ff\",\"#009292\",\"#ff6db6\",\"#924900\",\"#490092\",\"#006ddb\",\"#b66dff\",\"#004949\",\"#b6dbff\",\"#920000\",\"#ffb6db\",\"#db6d00\",\"#24ff24\",\"#ffff6d\"]\n",
    "# Plasma perceptually uniform - cmap = plt.cm.plasma\n",
    "# \"#F57D15\", \"#65156E\"\n",
    "\n",
    "dark_purple = \"#742881\"\n",
    "dark_green = \"#1B7939\"\n",
    "dark_blue = \"#1065AB\"\n",
    "dark_red = \"#BF2C23\"\n",
    "taupe = \"#B9A281\"\n",
    "sage_green = \"#0A5C36\"\n",
    "terracotta = \"#CB6843\"\n",
    "light_terracotta = \"#DA957B\"\n",
    "raspberry = \"#E30B5C\"\n",
    "brown = \"#A68A64\"\n",
    "dark_brown = \"#855e46\"\n",
    "yellow = \"#FDB338\"\n",
    "blue = \"#025196\"\n",
    "\n",
    "# Nice plasma\n",
    "# colors_histogram = [\"#D44842\", \"#65156E\"]\n",
    "# colors_scatter = [\"#D44842\", \"#65156E\"]\n",
    "\n",
    "# cmap colors\n",
    "choose_cmap = mpl.colormaps['turbo']\n",
    "hex_left = mpl.colors.rgb2hex(choose_cmap(0.05), keep_alpha=True)\n",
    "hex_right = mpl.colors.rgb2hex(choose_cmap(0.95), keep_alpha=True)\n",
    "\n",
    "colors_histogram = [hex_left, hex_right]\n",
    "\n",
    "# blue --> yellow\n",
    "# colors_histogram = [\"#DAA520\", \"#00346e\"] # yellow/blue\n",
    "# colors_histogram = [\"#936b09\", \"#00346e\"]\n",
    "# colors_scatter = [\"#DAA520\", \"#00346e\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_to_name = {}\n",
    "coord_to_name[0] = 'mchirp'\n",
    "coord_to_name[1] = 'q'\n",
    "coord_to_name[2] = 'distance'\n",
    "coord_to_name[3] = 'snr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_label = {}\n",
    "name_to_label['distance'] = r'$D$'\n",
    "name_to_label['snr'] = r'$\\rho$'\n",
    "name_to_label['q'] = r'$q$'\n",
    "name_to_label['mchirp'] = r'$\\mathcal{M}_c$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_names = ['1-per-month', '1-per-week', '1-per-day', '100-per-month', '1000-per-month']\n",
    "# How many signals are present above given threshold?\n",
    "for n, thresh in enumerate(team_1[\"far_thresholds\"]):\n",
    "    team_1[thresh_names[n]] = team_1['found_idx'][team_1[\"found_stats\"] > thresh]\n",
    "for n, thresh in enumerate(team_2[\"far_thresholds\"]):\n",
    "    team_2[thresh_names[n]] = team_2['found_idx'][team_2[\"found_stats\"] > thresh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_name = '1-per-month'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_params = ['mchirp', 'q', 'distance', 'snr']\n",
    "data_pycbc = {}\n",
    "data_sage = {}\n",
    "for param in plotting_params:\n",
    "    data_sage[param] = team_1[param][team_1[thresh_name]]\n",
    "    data_pycbc[param] = team_2[param][team_2[thresh_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hue_neg, hue_pos = 260, 15\n",
    "hue_neg, hue_pos = 260, 15\n",
    "cmap_diverging_dark = sns.diverging_palette(hue_neg, hue_pos, sep=1, center=\"dark\", as_cmap=True)\n",
    "print(cmap_diverging_dark) # this is a matplotlib object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_segmented = cmap_diverging_dark  # define the colormap\n",
    "# extract all colors from the .jet map\n",
    "cmaplist = [cmap_segmented(i) for i in range(cmap_segmented.N)]\n",
    "\n",
    "# create the new map\n",
    "cmap_segmented = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "            'Custom cmap', cmaplist, cmap_segmented.N)\n",
    "\n",
    "# define the bins and normalize\n",
    "# Should have -1 and 1 but not 0\n",
    "# pycbc - bounds = np.linspace(-11, 11, 12)\n",
    "bounds = np.arange(-22, 24, 2)\n",
    "print(bounds)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap_segmented.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListedColormap(cmap_diverging_dark(np.linspace(0.0, 0.50, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListedColormap(cmap_diverging_dark(np.linspace(0.50, 1.0, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListedColormap(cmap_diverging_dark(np.linspace(0.0, 1.0, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListedColormap(cmap_diverging_dark(np.linspace(0.0, 1.0, 21)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_function(z):\n",
    "    z = np.array(z)\n",
    "    num_pycbc = len(z[z == 0])\n",
    "    num_sage = len(z[z == 1])\n",
    "    return num_sage-num_pycbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.hstack((data_pycbc['mchirp'], data_sage['mchirp']))\n",
    "y_data = np.hstack((data_pycbc['q'], data_sage['q']))\n",
    "Z = np.hstack((np.full(len(data_pycbc['mchirp']), 0), np.full(len(data_sage['mchirp']), 1)))\n",
    "hexb = plt.hexbin(x_data, y_data, C=Z, reduce_C_function=reduce_function, gridsize=32)\n",
    "plt.close()\n",
    "\n",
    "positions = hexb.get_offsets()\n",
    "values = hexb.get_array()\n",
    "\n",
    "upper_hexbin = int(max(values))\n",
    "lower_hexbin = int(min(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom colormap based on limits provided\n",
    "# We need an odd number of colors to include zero difference\n",
    "## CHANGE LIMITS HERE!!!\n",
    "lower_hexbin, upper_hexbin = (-11, 6)\n",
    "limits = (lower_hexbin, upper_hexbin)\n",
    "lower = np.linspace(0.0, 0.40, abs(limits[0]))\n",
    "upper = np.linspace(0.60, 1.0, abs(limits[1]))\n",
    "full = np.concatenate((lower, [0.5], upper))\n",
    "print(full)\n",
    "\n",
    "required = full[:]\n",
    "ListedColormap(cmap_diverging_dark(required))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_diverging_cmap = ListedColormap(cmap_diverging_dark(required))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors1 = plt.cm.viridis_r(np.linspace(0.0, 0.5, abs(limits[0])))\n",
    "colors2 = plt.cm.plasma(np.linspace(0.0, 0.5, abs(limits[1])))\n",
    "black = cmap_diverging_dark([0.5])\n",
    "\n",
    "# combine them and build a new colormap\n",
    "colors = np.vstack((colors1, black, colors2))\n",
    "custom = mcolors.LinearSegmentedColormap.from_list('my_colormap', colors)\n",
    "ListedColormap(custom(np.linspace(0, 1, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_perceptually_uniform = ListedColormap(custom(np.linspace(0, 1, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "x_data = np.hstack((data_pycbc['mchirp'], data_sage['mchirp']))\n",
    "y_data = np.hstack((data_pycbc['q'], data_sage['q']))\n",
    "bins = np.histogram(np.hstack((data_pycbc['mchirp'], data_sage['mchirp'])), bins=40)[1]\n",
    "ax.hist(data_pycbc['mchirp'], density=False, bins=bins, alpha=0.8, histtype='step', color=colors_histogram[0], linewidth=2.0, linestyle='dashed', label='PyCBC')\n",
    "ax.hist(data_sage['mchirp'], density=False, bins=bins, alpha=0.8, histtype='step', color=colors_histogram[1], linewidth=2.0, linestyle='solid', label='Sage')\n",
    "\n",
    "plt.xlabel('Chirp Mass (in Solar mass)')\n",
    "plt.ylabel('Number of Detected Signals')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "x_data = np.hstack((data_pycbc['mchirp'], data_sage['mchirp']))\n",
    "y_data = np.hstack((data_pycbc['q'], data_sage['q']))\n",
    "Z = np.hstack((np.full(len(data_pycbc['mchirp']), 0), np.full(len(data_sage['mchirp']), 1)))\n",
    "# rf = partial(reduce_function, arg=arg)\n",
    "# hexb = plt.hexbin(x_data, y_data, C=Z, reduce_C_function=reduce_function, gridsize=32, cmap=cmap_segmented, vmin=-5, vmax=22)\n",
    "# hexb = plt.hexbin(x_data, y_data, C=Z, reduce_C_function=reduce_function, gridsize=32, cmap=cmap_segmented, norm=norm)\n",
    "hexb = plt.hexbin(x_data, y_data, C=Z, reduce_C_function=reduce_function, gridsize=32, cmap=custom_diverging_cmap, norm=mpl.colors.Normalize(vmin=lower_hexbin-0.5,vmax=upper_hexbin+0.5))\n",
    "cbar = plt.colorbar(hexb, ticks=np.arange(lower_hexbin, upper_hexbin+1, 1))\n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "plt.xlabel(r'$\\mathcal{M}_c$')\n",
    "plt.ylabel(r'$q$')\n",
    "\n",
    "positions = hexb.get_offsets()\n",
    "values = hexb.get_array()\n",
    "\n",
    "print(max(values), min(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(8, 7.5))\n",
    "gs = gridspec.GridSpec(2, 2, width_ratios=[4,1], height_ratios=[1,4], wspace=0, hspace=0)\n",
    "\n",
    "# 2D hexbin (main)\n",
    "ax_hex = plt.subplot(gs[2])\n",
    "x_data = np.hstack((data_pycbc['mchirp'], data_sage['mchirp']))\n",
    "y_data = np.hstack((data_pycbc['q'], data_sage['q']))\n",
    "Z = np.hstack((np.full(len(data_pycbc['mchirp']), 0), np.full(len(data_sage['mchirp']), 1)))\n",
    "hexb = ax_hex.hexbin(x_data, y_data, C=Z, reduce_C_function=reduce_function, gridsize=32, cmap=custom_diverging_cmap, norm=mpl.colors.Normalize(vmin=lower_hexbin-0.5,vmax=upper_hexbin+0.5))\n",
    "ax_hex.set_xlabel(r'$\\mathcal{M}_c\\;(\\mathregular{in}\\;\\mathregular{M}_\\odot)$')\n",
    "ax_hex.set_ylabel(r'$q$')\n",
    "\n",
    "ax_hex.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "ax_hex.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "ax_hex.tick_params(axis=\"x\",which='minor',length=2.5,bottom=True,left=True,top=True,right=True,direction='in')\n",
    "ax_hex.tick_params(axis=\"y\",which='minor',length=2.5,bottom=True,left=True,top=True,right=True,direction='in')\n",
    "ax_hex.tick_params(axis=\"x\",which='major',length=5,bottom=True,left=True,top=True,right=True,direction='in')\n",
    "ax_hex.tick_params(axis=\"y\",which='major',length=5,bottom=True,left=True,top=True,right=True,direction='in')\n",
    "\n",
    "# 1D histogram (dim x)\n",
    "ax_dim1 = plt.subplot(gs[0])\n",
    "bins = np.histogram(np.hstack((data_pycbc['mchirp'], data_sage['mchirp'])), bins=40)[1]\n",
    "ax_dim1.hist(data_pycbc['mchirp'], density=False, bins=bins, alpha=0.8, histtype='step', color=colors_histogram[0], linewidth=1.5, linestyle='dashed')\n",
    "ax_dim1.hist(data_sage['mchirp'], density=False, bins=bins, alpha=1.0, histtype='step', color=colors_histogram[1], linewidth=1.5, linestyle='solid')\n",
    "# Cleanin up 1D histogram subplots\n",
    "ax_dim1.set_xticklabels([])\n",
    "ax_dim1.set_yticklabels([])\n",
    "ax_dim1.tick_params(bottom=False, left=False, right=False, top=False)\n",
    "ax_dim1.spines['top'].set_visible(False)\n",
    "ax_dim1.spines['right'].set_visible(False)\n",
    "ax_dim1.spines['bottom'].set_visible(False)\n",
    "ax_dim1.spines['left'].set_visible(False)\n",
    "\n",
    "# 1D histogram (dim y)\n",
    "ax_dim2 = plt.subplot(gs[3])\n",
    "bins = np.histogram(np.hstack((data_pycbc['q'], data_sage['q'])), bins=40)[1]\n",
    "ax_dim2.hist(data_pycbc['q'], density=False, bins=bins, alpha=0.8, histtype='step', color=colors_histogram[0], linewidth=1.5, linestyle='dashed', orientation='horizontal')\n",
    "ax_dim2.hist(data_sage['q'], density=False, bins=bins, alpha=1.0, histtype='step', color=colors_histogram[1], linewidth=1.5, linestyle='solid', orientation='horizontal')\n",
    "# Cleanin up 1D histogram subplots\n",
    "ax_dim2.set_xticklabels([])\n",
    "ax_dim2.set_yticklabels([])\n",
    "ax_dim2.tick_params(bottom=False, left=False, right=False, top=False)\n",
    "ax_dim2.spines['top'].set_visible(False)\n",
    "ax_dim2.spines['right'].set_visible(False)\n",
    "ax_dim2.spines['bottom'].set_visible(False)\n",
    "\n",
    "# Legend for 1D histograms\n",
    "sage_line = mlines.Line2D([], [], color=colors_histogram[1], label='Sage', linewidth=2.0)\n",
    "pycbc_line = mlines.Line2D([], [], color=colors_histogram[0], label='PyCBC', linewidth=2.0, linestyle='dashed')\n",
    "# left/right, up/down, width, height\n",
    "plt.legend(handles=[sage_line, pycbc_line], bbox_to_anchor=(-1.8, 0.6, 0.5, 0.5), loc='center left', frameon=False, handlelength=2.6)\n",
    "\n",
    "# Other plotting params\n",
    "# left/right, up/down, width, height\n",
    "cax = ax_hex.inset_axes([5.0, -0.8, 40.0, 0.4], transform=ax_hex.transData)\n",
    "cbar = fig.colorbar(hexb, cax=cax, orientation='horizontal', ticks=np.arange(lower_hexbin, upper_hexbin+1, 1.0))\n",
    "cbar.set_label(r'$\\mathregular{N^F_{sage} - N^F_{pycbc}}$')\n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "tick_rotation = 0\n",
    "plt.setp(cbar.ax.get_xticklabels(), rotation=tick_rotation, horizontalalignment='right')\n",
    "\n",
    "plt.savefig(\"./evaluation_plots/compare_hexbinned_2params.png\", bbox_inches='tight', dpi=500)\n",
    "\n",
    "positions = hexb.get_offsets()\n",
    "values = hexb.get_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should include a zero\n",
    "# pycbc - np.linspace(-12.0, 12.0, 13)\n",
    "np.arange(-5.0, 24.0, 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure and axes\n",
    "num_rows = len(data_sage.keys())\n",
    "num_cols = len(data_sage.keys())\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(4*num_rows, 4*num_cols), facecolor='white')\n",
    "gs = gridspec.GridSpec(num_rows, num_cols, wspace=0, hspace=0)\n",
    "tick_rotation = 45.0 # degrees\n",
    "flag = True\n",
    "    \n",
    "coords = itertools.product(np.arange(num_rows), np.arange(num_cols))\n",
    "names = itertools.product(list(data_sage.keys()), list(data_sage.keys()))\n",
    "for name, coord in zip(names, coords):\n",
    "    param_1, param_2 = name\n",
    "    if coord[0] == coord[1]:\n",
    "        assert param_1 == param_2\n",
    "        paxis = plt.subplot(gs[int(4*coord[0]+coord[1])])\n",
    "        ax = paxis\n",
    "        # Make histogram\n",
    "        bins = np.histogram(np.hstack((data_pycbc[param_1], data_sage[param_1])), bins=40)[1]\n",
    "        ax.hist(data_pycbc[param_1], density=False, bins=bins, alpha=0.8, histtype='step', color=colors_histogram[0], linewidth=2.0, linestyle='dashed')\n",
    "        ax.hist(data_sage[param_1], density=False, bins=bins, alpha=1.0, histtype='step', color=colors_histogram[1], linewidth=2.0, linestyle='solid')\n",
    "\n",
    "        plt.setp(ax.get_xticklabels(), rotation=tick_rotation, horizontalalignment='right')\n",
    "        plt.setp(ax.get_yticklabels(), rotation=tick_rotation, horizontalalignment='right')\n",
    "\n",
    "        if coord[1] == 0:\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "        elif coord[0] == num_cols-1:\n",
    "            paxis.set_xlabel(name_to_label[coord_to_name[coord[1]]])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_yticks([])\n",
    "        else:\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "    elif coord[1] > coord[0]:\n",
    "        ax = plt.subplot(gs[int(4*coord[0]+coord[1])])\n",
    "        ax.remove()\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        ppaxis = plt.subplot(gs[int(4*coord[0]+coord[1])])\n",
    "        # marker = '+' if n==0 else 'o'\n",
    "        # s = 20 if n==0 else 25\n",
    "        # kwargs = {'color':colors_scatter[n]} if n==0 else {'edgecolors':colors_scatter[n], 'facecolors':'none'}\n",
    "        # ppaxis.scatter(p2_data, p1_data, s=s, alpha=0.8, marker=marker, **kwargs)\n",
    "\n",
    "        ##  Replacing the scatter plot with hexbinned color plot\n",
    "        x_data = np.hstack((data_pycbc[coord_to_name[coord[0]]], data_sage[coord_to_name[coord[0]]]))\n",
    "        y_data = np.hstack((data_pycbc[coord_to_name[coord[1]]], data_sage[coord_to_name[coord[1]]]))\n",
    "        Z = np.hstack((np.full(len(data_pycbc[coord_to_name[coord[0]]]), 0), np.full(len(data_sage[coord_to_name[coord[0]]]), 1)))\n",
    "        hexb = plt.hexbin(y_data, x_data, C=Z, reduce_C_function=reduce_function, gridsize=32, cmap=custom_diverging_cmap, norm=mpl.colors.Normalize(vmin=lower_hexbin-0.5,vmax=upper_hexbin+0.5))\n",
    "        values = hexb.get_array()\n",
    "        print(min(values), max(values))\n",
    "\n",
    "        if flag:\n",
    "            # left/right, up/down, width, height\n",
    "            cax = ppaxis.inset_axes([150.0, 5.0, 3.0, 7.0], transform=ppaxis.transData)\n",
    "            cbar = fig.colorbar(hexb, cax=cax, orientation='vertical', ticks=np.arange(lower_hexbin, upper_hexbin+1, 1.0))\n",
    "            cbar.set_label(r'$\\mathregular{N^F_{sage} - N^F_{pycbc}}$', rotation=90)\n",
    "            cbar.ax.tick_params(labelsize=12)\n",
    "            flag = False\n",
    "\n",
    "        name = 'PyCBC' if n==0 else 'Sage'\n",
    "        \"\"\"\n",
    "        corner.hist2d(p2_data, p1_data,\n",
    "                    ax=ppaxis, color=colors_fill[n],\n",
    "                    levels=[0.95, 0.68], bins=512, smooth=42.,\n",
    "                    plot_datapoints=False, plot_contours=True, fill_contours=False,\n",
    "                    plot_density=False, contour_kwargs=contour_dict[name])\n",
    "        \"\"\"\n",
    "        # Rotate tick labels\n",
    "        plt.setp(ppaxis.get_xticklabels(), rotation=tick_rotation, horizontalalignment='right')\n",
    "        plt.setp(ppaxis.get_yticklabels(), rotation=tick_rotation, horizontalalignment='right')\n",
    "\n",
    "        if coord[1] == 0:\n",
    "            ppaxis.set_ylabel(name_to_label[param_1])\n",
    "            if coord[0] != num_rows - 1:\n",
    "                ppaxis.set_xticklabels([])\n",
    "                ppaxis.set_xticks([])\n",
    "        if coord[0] == num_cols-1:\n",
    "            ppaxis.set_xlabel(name_to_label[param_2])\n",
    "            if coord[1] != 0:\n",
    "                ppaxis.set_yticklabels([])\n",
    "                ppaxis.set_yticks([])\n",
    "        if coord[1] !=0 and coord[0] != num_cols-1:\n",
    "            ppaxis.set_xticklabels([])\n",
    "            ppaxis.set_yticklabels([])\n",
    "            ppaxis.set_xticks([])\n",
    "            ppaxis.set_yticks([])\n",
    "\n",
    "## save the plot\n",
    "fig.subplots_adjust(wspace=0)\n",
    "fig.subplots_adjust(hspace=0)\n",
    "# Legend\n",
    "sage_line = mlines.Line2D([], [], color=colors_histogram[1], label='Sage', linewidth=4.0)\n",
    "pycbc_line = mlines.Line2D([], [], color=colors_histogram[0], label='PyCBC', linewidth=4.0, linestyle='dashed')\n",
    "# left/right, up/down, width, height\n",
    "plt.legend(handles=[sage_line, pycbc_line], bbox_to_anchor=(-0.45, 0.33, 1., 6.5), loc='center left', frameon=False, fontsize=22, handlelength=2.6)\n",
    "# Found and missed numbers\n",
    "sage_set = set(data_sage['mchirp'])\n",
    "pycbc_set = set(data_pycbc['mchirp'])\n",
    "found_by_both = len(set.intersection(sage_set, pycbc_set))\n",
    "found_only_by_sage = len(sage_set - pycbc_set)\n",
    "found_only_by_pycbc = len(pycbc_set - sage_set)\n",
    "plt.text(0.35, 3.2, 'Found by both = {}'.format(str(found_by_both)), transform=ax.transAxes, horizontalalignment='right', fontsize=15)\n",
    "plt.text(0.35, 3.2-0.125, 'Found only by Sage = {}'.format(str(found_only_by_sage)+'  '), transform=ax.transAxes, horizontalalignment='right', fontsize=15)\n",
    "plt.text(0.35, 3.2-0.250, 'Found only by PyCBC = {}'.format(str(found_only_by_pycbc)+'  '), transform=ax.transAxes, horizontalalignment='right', fontsize=15)\n",
    "\n",
    "plt.savefig(\"./evaluation_plots/compare_hexbinned.png\", bbox_inches='tight', dpi=500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making found/missed histograms at different FARs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(output_dir, \"histogram\")\n",
    "\n",
    "# Plot 1 (Histogram of all injections with found injections of both pipelines)\n",
    "os.makedirs(save_dir, exist_ok=False)\n",
    "# params = team_1['params']\n",
    "params = ['mass1', 'mass2', 'mchirp',\n",
    "          'q', 'distance', 'chirp_distance',\n",
    "          'coa_phase', 'inclination', 'polarization',\n",
    "          'ra', 'dec', 'snr',\n",
    "          'spin1x', 'spin1y', 'spin1z',\n",
    "          'spin2x', 'spin2y', 'spin2z']\n",
    "\n",
    "ncols = 3\n",
    "nrows = len(params)//ncols + int(len(params)%ncols or False)\n",
    "\n",
    "thresh_names = ['1-per-month', '1-per-week', '1-per-day', '100-per-month', '1000-per-month']\n",
    "# How many signals are present above given threshold?\n",
    "for n, thresh in enumerate(team_1[\"far_thresholds\"]):\n",
    "    team_1[thresh_names[n]] = team_1['found_idx'][team_1[\"found_stats\"] > thresh]\n",
    "for n, thresh in enumerate(team_2[\"far_thresholds\"]):\n",
    "    team_2[thresh_names[n]] = team_2['found_idx'][team_2[\"found_stats\"] > thresh]\n",
    "\n",
    "for thresh_name in thresh_names:\n",
    "    # Subplotting\n",
    "    fig, ax = plt.subplots(nrows, ncols, figsize=(8.0*ncols, 6.0*nrows))\n",
    "    # Histogram kwargs\n",
    "    kwargs = dict(histtype='stepfilled', alpha=0.5)\n",
    "    \n",
    "    pidxs = list(itertools.product(range(nrows), range(ncols)))\n",
    "    num_fin = 0\n",
    "    for param, (i, j)  in zip(params, pidxs):\n",
    "        bins = np.histogram(np.hstack((team_2[param][team_2[thresh_name]], team_1[param][team_1[thresh_name]])), bins=40)[1]\n",
    "        ax[i][j].hist(team_2[param][team_2[thresh_name]], bins=bins, alpha=0.8, histtype='step', color=colors_histogram[0], linewidth=2.0, linestyle='dashed')\n",
    "        ax[i][j].hist(team_1[param][team_1[thresh_name]], bins=bins, alpha=1.0, histtype='step', color=colors_histogram[1], linewidth=2.0, linestyle='solid')\n",
    "        ax[i][j].set_title(param)\n",
    "        ax[i][j].grid(True)\n",
    "        ax[i][j].legend()\n",
    "        num_fin+=1\n",
    "    \n",
    "    for i, j in pidxs[num_fin:]:\n",
    "        ax[i][j].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_name = \"compare_histogram_{}_and_{}-{}.png\".format(team_1['name'], team_2['name'], thresh_name)\n",
    "    save_path = os.path.join(save_dir, save_name)\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitive Distance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "stats = ret\n",
    "with h5py.File(\"./evaluation_plots/evaluation_output.hdf\", 'w') as fp:\n",
    "    for key, val in stats.items():\n",
    "        fp.create_dataset(key, data=np.array(val))\n",
    "\n",
    "# Create the sensitivity vs FAR/month plot from the output evaluation obtained\n",
    "assert dur == far_scaling_factor, 'FAR scaling factor discrepancy! Check duration.'\n",
    "with h5py.File(\"./evaluation_plots/evaluation_output.hdf\", 'r') as fp:\n",
    "    far = fp['far'][()]\n",
    "    sens = fp['sensitive-distance'][()]\n",
    "    sidxs = far.argsort()\n",
    "    far = far[sidxs][1:] * far_scaling_factor\n",
    "    sens = sens[sidxs][1:]\n",
    "    \n",
    "    far_pycbc = fp['far-pycbc'][()]\n",
    "    sidxs_pycbc = far_pycbc.argsort()\n",
    "    far_pycbc_chk = far_pycbc[sidxs_pycbc] * far_scaling_factor\n",
    "    sens_pycbc_check = fp['sensitive-distance-pycbc'][()]\n",
    "    sens_pycbc_check = sens_pycbc_check[sidxs_pycbc]\n",
    "\n",
    "# Month FAR factor\n",
    "month = 30.0 * 24.0 * 60.0 * 60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12.0, 8.0))\n",
    "plt.title('Sensitivity Measure for Dataset {}'.format(dataset))\n",
    "plt.plot(far*(month/dur), sens, color=sage_green, linewidth=2.5, label='Sage')\n",
    "\n",
    "with h5py.File('/home/nnarenraju/Research/ORChiD/results/PyCBC/ds{}/eval.hdf'.format(dataset)) as fp:\n",
    "    sens_pycbc = np.array(fp['sensitive-distance'])\n",
    "    far_pycbc = np.array(fp['far'])\n",
    "plt.plot(far_pycbc*month, sens_pycbc, color=dark_brown, linewidth=2.0, linestyle='dashed', label='PyCBC')\n",
    "\n",
    "with h5py.File('/home/nnarenraju/Research/ORChiD/results/TPI_FSU_Jena/ds{}/eval.hdf'.format(dataset)) as fp:\n",
    "    sens_fsu = np.array(fp['sensitive-distance'])\n",
    "    far_fsu = np.array(fp['far'])\n",
    "plt.plot(far_fsu*month, sens_fsu, color='red', linewidth=2.0, linestyle='dashed', label='TPI FSU Jena')\n",
    "\n",
    "with h5py.File('/home/nnarenraju/Research/ORChiD/results/Virgo-AUTh/ds{}/eval.hdf'.format(dataset)) as fp:\n",
    "    sens_virgo = np.array(fp['sensitive-distance'])\n",
    "    far_virgo = np.array(fp['far'])\n",
    "plt.plot(far_virgo*month, sens_virgo, color='blueviolet', linewidth=2.0, linestyle='dashed', label='Virgo-AUTh')\n",
    "\n",
    "with h5py.File('/home/nnarenraju/Research/ORChiD/results/CNN-Coinc/ds{}/eval.hdf'.format(dataset)) as fp:\n",
    "    sens_cnn = np.array(fp['sensitive-distance'])\n",
    "    far_cnn = np.array(fp['far'])\n",
    "plt.plot(far_cnn*month, sens_cnn, color='m', linewidth=2.0, linestyle='dashed', label='CNN-Coinc')\n",
    "\n",
    "with h5py.File('/home/nnarenraju/Research/ORChiD/results/MFCNN/ds{}/eval.hdf'.format(dataset)) as fp:\n",
    "    sens_mfcnn = np.array(fp['sensitive-distance'])\n",
    "    far_mfcnn = np.array(fp['far'])\n",
    "plt.plot(far_mfcnn*month, sens_mfcnn, color='orange', linewidth=2.0, linestyle='dashed', label='MFCNN')\n",
    "\n",
    "plt.grid(True, which='both')\n",
    "plt.xlim(1000, 1)\n",
    "plt.ylim(0, 2500)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('False Alarm Rate (FAR) per month')\n",
    "plt.ylabel('Sensitive Distance [MPc]')\n",
    "plt.legend()\n",
    "plt.savefig(\"./evaluation_plots/sensitive_distance_all_teams.png\", bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12.0, 8.0))\n",
    "# \"#D44842\", \"#65156E\"\n",
    "plt.plot(far*(month/dur), sens, color=\"#65156E\", linewidth=3.0, label='Sage')\n",
    "\n",
    "with h5py.File('/home/nnarenraju/Research/ORChiD/results/{}/ds{}/eval.hdf'.format('PyCBC', dataset)) as fp:\n",
    "    sens_team2 = np.array(fp['sensitive-distance'])\n",
    "    far_team2 = np.array(fp['far'])\n",
    "    plt.plot(far_team2*month, sens_team2, color=\"#D44842\", linewidth=2.0, linestyle='dashed', label='{}'.format('PyCBC'))\n",
    "\n",
    "plt.grid(True, which='both')\n",
    "plt.xlim(1000, 1)\n",
    "plt.ylim(1500, 2100)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('False Alarm Rate (FAR) per month')\n",
    "plt.ylabel('Sensitive Distance [MPc]')\n",
    "plt.legend()\n",
    "plt.savefig(\"./evaluation_plots/sensitive_distance_pycbc_sage.png\", bbox_inches='tight', dpi=300)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure(title=\"\"):\n",
    "    # plt.rc('font', family='serif')\n",
    "    plt.rc('xtick', labelsize='medium')\n",
    "    plt.rc('ytick', labelsize='medium')\n",
    "    fig, axs = plt.subplots(1, figsize=(16.0, 14.0))\n",
    "    fig.suptitle(title, fontsize=18, y=0.92)\n",
    "    return axs, fig\n",
    "\n",
    "\n",
    "def _plot(ax, x=None, y=None, xlabel=\"x-axis\", ylabel=\"y-axis\", ls='solid', \n",
    "          label=\"NULL\", c=None, yscale='linear', xscale='linear', histogram=False):\n",
    "    \n",
    "    # Plotting type\n",
    "    if histogram:\n",
    "        ax.hist(y, bins=100, label=label, alpha=0.8)\n",
    "    else:\n",
    "        ax.plot(x, y, ls=ls, c=c, linewidth=3.0, label=label)\n",
    "    \n",
    "    # Plotting params\n",
    "    ax.set_xscale(xscale)\n",
    "    ax.set_yscale(yscale)\n",
    "    ax.grid(True, which='both')\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_network_output(fgpath, bgpath):\n",
    "    with h5py.File(fgpath, 'r') as fp:\n",
    "        fg_events = [np.vstack([fp['time'], fp['stat'], fp['var']])]\n",
    "    fg_events = np.concatenate(fg_events, axis=-1)\n",
    "    sidxs = fg_events[0].argsort()\n",
    "    fg_events = fg_events.T[sidxs].T\n",
    "    \n",
    "    # Read background events\n",
    "    with h5py.File(bgpath, 'r') as fp:\n",
    "        bg_events = [np.vstack([fp['time'], fp['stat'], fp['var']])]\n",
    "    bg_events = np.concatenate(bg_events, axis=-1)\n",
    "    sidxs = bg_events[0].argsort()\n",
    "    bg_events = bg_events.T[sidxs].T\n",
    "\n",
    "    return fg_events, bg_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Foreground and background dataset outputs from pipeline\n",
    "teams = {'PyCBC': {'fg_events': None, 'bg_events': None}, \n",
    "         'Sage': {'fg_events': None, 'bg_events': None}}\n",
    "\n",
    "# Paths\n",
    "# fgpath = [\"/home/nnarenraju/Research/ORChiD/results/PyCBC/ds4/fg.hdf\", foutput]\n",
    "# bgpath = [\"/home/nnarenraju/Research/ORChiD/results/PyCBC/ds4/bg.hdf\", boutput]\n",
    "fout = foutput\n",
    "bout = boutput\n",
    "fgpath = [\"/home/nnarenraju/Research/ORChiD/results/PyCBC/ds4/fg.hdf\", fout]\n",
    "bgpath = [\"/home/nnarenraju/Research/ORChiD/results/PyCBC/ds4/bg.hdf\", bout]\n",
    "# Get data from the outputs files\n",
    "for fgp, bgp, tname in zip(fgpath, bgpath, teams.keys()):\n",
    "    fg_events, bg_events = read_network_output(fgp, bgp)\n",
    "    teams[tname]['bg_events'] = bg_events\n",
    "    teams[tname]['fg_events'] = fg_events\n",
    "\n",
    "## Injections file and foreground file data\n",
    "injfile = injections\n",
    "fgfile = foreground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all valid injection times for the given testing dataset\n",
    "duration, valid_idxs = find_injection_times([fgfile], injfile)\n",
    "# Read the injections file and get the values of injection params\n",
    "print(\"Reading the injections file for testing dataset\")\n",
    "assert all(team_1['tc'] == team_2['tc'])\n",
    "tc = team_1['tc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nclosest_index(trigger_times, injection_times, frange=[-0.3, +0.3]):\n",
    "    # Find nclosest triggers given an injection time\n",
    "    if len(trigger_times) == 0:\n",
    "        raise ValueError('Cannot find closest index for empty trigger times array.')\n",
    "    \n",
    "    # Sort array before proceeding\n",
    "    array = trigger_times.copy()\n",
    "    array.sort()\n",
    "    \n",
    "    # Find nclosest indices for the given value in array\n",
    "    tidxs = [np.argwhere((array >= inj + frange[0]) & (array <= inj + frange[1])) for inj in tqdm(injection_times)]\n",
    "    return tidxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax, _ = figure(title=None)\n",
    "\n",
    "# Save ROC data\n",
    "save = {}\n",
    "save['Sage'] = {'fpr': None, 'tpr': None}\n",
    "save['PyCBC'] = {'fpr': None, 'tpr': None}\n",
    "\n",
    "for tname in teams.keys():\n",
    "    print(\"\\nComputing ROC Curve for {}\".format(tname))\n",
    "    fg_events = teams[tname]['fg_events']\n",
    "    bg_events = teams[tname]['bg_events']\n",
    "\n",
    "    ## Outputs and Labels for FG and BG\n",
    "    outputs = []\n",
    "    labels = []\n",
    "    \n",
    "    \"\"\" Foreground file analysis \"\"\"\n",
    "    print(\"\\nAnalysing the foreground data\")\n",
    "    # Read the foreground output and get all the trigger times found\n",
    "    fg_trigger_times = fg_events[0]\n",
    "    fg_trigger_stats = fg_events[1]\n",
    "\n",
    "    # Given all the valid injection times present in the injections file\n",
    "    # get idx of all triggers that are within a bound of the given injection time\n",
    "    print(\"Getting closest trigger indices for each valid injection time\")\n",
    "    fg_bound_triggers_idx = find_nclosest_index(trigger_times=fg_trigger_times, injection_times=injparams['tc'], frange=[-0.3, +0.3])\n",
    "    \n",
    "    print(\"Maximising over each set of triggers and obtaining the output and labels for ROC curve\")\n",
    "    ## Get outputs and labels to get ROC curve\n",
    "    pbar = tqdm(fg_bound_triggers_idx)\n",
    "    for btidx in pbar:\n",
    "        pbar.set_description(\"Maximising FG triggers\")\n",
    "        if len(btidx) > 0:\n",
    "            # Maximise on these triggers to get the event\n",
    "            outputs.append(np.max(fg_trigger_stats[btidx]))\n",
    "        else:\n",
    "            # If there are no triggers given, then we assign 0.0 stat for event\n",
    "            outputs.append(-np.inf)\n",
    "        # Append labels as 1.0 since only GW events are looked at\n",
    "        labels.append(1.0)\n",
    "    \n",
    "    \"\"\" Background file analysis \"\"\"\n",
    "    print(\"\\nAnalysing the background data\")\n",
    "    # Read the foreground output and get all the trigger times found\n",
    "    bg_trigger_times = bg_events[0]\n",
    "    bg_trigger_stats = bg_events[1]\n",
    "    \n",
    "    # Given all the valid injection times present in the injections file\n",
    "    # get idx of all triggers that are within a bound of the given injection time\n",
    "    print(\"Getting closest trigger indices for each valid injection time\")\n",
    "    bg_bound_triggers_idx = find_nclosest_index(trigger_times=bg_trigger_times, injection_times=injparams['tc'], frange=[-0.3, +0.3])\n",
    "    \n",
    "    print(\"Maximising over each set of triggers and obtaining the output and labels for ROC curve\")\n",
    "    ## Get outputs and labels to get ROC curve\n",
    "    pbar = tqdm(bg_bound_triggers_idx)\n",
    "    for btidx in pbar:\n",
    "        pbar.set_description(\"Maximising BG triggers\")\n",
    "        if len(btidx) > 0:\n",
    "            # Maximise on these triggers to get the event\n",
    "            outputs.append(np.max(bg_trigger_stats[btidx]))\n",
    "        else:\n",
    "            # If there are no triggers given, then we assign 0.0 stat for event\n",
    "            outputs.append(-np.inf)\n",
    "        # Append labels as 0.0 since only noise data is looked at\n",
    "        labels.append(0.0)\n",
    "\n",
    "    \"\"\" Calculating the ROC Curve \"\"\"\n",
    "    outputs = np.array(outputs)\n",
    "    labels = np.array(labels)\n",
    "    # Convert the outputs from raw outputs to prediction probabilites using sigmoid\n",
    "    sigmoid = lambda x: 1./(1. + np.exp(-1*x))\n",
    "    outputs = sigmoid(outputs)\n",
    "    # Calculate the ROC curve\n",
    "    fpr, tpr, threshold = metrics.roc_curve(labels, outputs)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    # Log ROC Curve\n",
    "    if tname == \"PyCBC\":\n",
    "        colour = 'red'\n",
    "    elif tname == \"Sage\":\n",
    "        colour = \"blue\"\n",
    "\n",
    "    # Save data\n",
    "    save[tname]['fpr'] = fpr\n",
    "    save[tname]['tpr'] = tpr\n",
    "    _plot(ax, fpr, tpr, label=\"{}, AUC = {}\".format(tname, np.around(roc_auc, 4)), c=colour, \n",
    "        ylabel=\"True Positive Rate\", xlabel=\"False Positive Rate\", \n",
    "        yscale='log', xscale='log')\n",
    "\n",
    "# Other plotting stuff\n",
    "_plot(ax, [0, 1], [0, 1], label=\"Random Classifier\", c='k', \n",
    "    ylabel=\"True Positive Rate\", xlabel=\"False Positive Rate\", \n",
    "    ls=\"dashed\", yscale='log', xscale='log')\n",
    "\n",
    "save_path = \"./evaluation_plots/roc_curve.png\"\n",
    "plt.savefig(save_path)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"font.size\": 18\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(12.0, 8.0))\n",
    "# \"#D44842\", \"#65156E\"\n",
    "plt.loglog(save['Sage']['fpr'], save['Sage']['tpr'], c=\"#65156E\", label='Sage', linewidth=2.0)\n",
    "plt.loglog(save['PyCBC']['fpr'], save['PyCBC']['tpr'], c=\"#D44842\", label='PyCBC', linewidth=2.0)\n",
    "plt.plot([0, 1], [0, 1], label='Random Classifier', c='k', linestyle='dashed')\n",
    "plt.xlabel('False Alarm Probability')\n",
    "plt.ylabel('True Alarm Probability')\n",
    "plt.xlim(0.0, 1.0)\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.grid(which='both')\n",
    "plt.legend(loc='lower right')\n",
    "save_path = \"./evaluation_plots/roc_pycbc_sage.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glitch Rejection Capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glitch_names = ['1400Ripples', '1080Lines', 'Air_Compressor', 'Blip', 'Chirp', 'Extremely_Loud', 'Helix',\n",
    "                'Koi_Fish', 'Light_Modulation', 'Low_Frequency_Burst', 'Low_Frequency_Lines', 'No_Glitch', \n",
    "                'None_of_the_Above', 'Paired_Doves', 'Power_Line', 'Repeating_Blips', 'Scattered_Light', \n",
    "                'Scratchy', 'Tomte', 'Violin_Mode', 'Wandering_Line', 'Whistle']\n",
    "glitch_alias = ['1400R', '1080L', 'AC', 'B', 'C', 'EL', 'H',\n",
    "                'KF', 'LM', 'LFB', 'LFL', 'NG', \n",
    "                'NA', 'PD', 'PL', 'RB', 'SL', \n",
    "                'S', 'T', 'VM', 'WL', 'W']\n",
    "\n",
    "map_to_idx = dict(zip(glitch_names, np.arange(len(glitch_names))))\n",
    "map_to_names = dict(zip(np.arange(len(glitch_names)), glitch_names))\n",
    "names_to_alias = dict(zip(glitch_names, glitch_alias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(glitch_alias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glitch_data(detfile):\n",
    "    # File containing glitch info H1 O3a\n",
    "    filename = \"/local/scratch/igr/nnarenraju/gwspy/{}.csv\".format(detfile)\n",
    "    gfile = pd.read_csv('{}'.format(filename))\n",
    "    num_glitches_in_odet = len(gfile)\n",
    "    print('H1 O3a contains {} glitches'.format(num_glitches_in_odet))\n",
    "    glitch_probs = gfile.iloc[:, 15:37]\n",
    "    glitch_types = []\n",
    "    for i in range(num_glitches_in_odet):\n",
    "        glitch_types.append(map_to_idx[glitch_probs.iloc[i].idxmax()])\n",
    "\n",
    "    glitch_gps_times = np.array(gfile['start_time'])\n",
    "    glitch_duration = np.array(gfile['duration'])\n",
    "    glitch_snrs = np.array(gfile['snr'])\n",
    "    return (glitch_gps_times, glitch_duration, glitch_snrs, glitch_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, glitch_types = get_glitch_data('H1_O3a')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "glitch_ids = [map_to_idx[foo] for foo in glitch_types]\n",
    "glitch_ids_freqs = {glitch_names[foo]: glitch_ids.count(foo) for foo in np.arange(len(glitch_names))}\n",
    "for key, val in glitch_ids_freqs.items():\n",
    "    print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glitches_in_testset(glitch_gps_times, glitch_duration, glitch_snrs, glitch_types, segments_csv='./tmp/segments.csv'):\n",
    "    # We use the first 133 noise segments as testing noise\n",
    "    # This contains start and end times\n",
    "    noise_times = pd.read_csv(segments_csv)[133:]\n",
    "    # Check if a glitch reported in gravity spy is present within the start and end times of the noise segments\n",
    "    num_present = 0\n",
    "    glitch_gps_times_in_testset = []\n",
    "    glitch_durations_in_testset = []\n",
    "    glitch_snrs_in_testset = []\n",
    "    glitch_types_in_testset = []\n",
    "    for glitch_time, gdur, gsnr, gtype in tqdm(zip(glitch_gps_times, glitch_duration, glitch_snrs, glitch_types)):\n",
    "        is_present = np.any((noise_times['start'] <= glitch_time) & (noise_times['end'] >= glitch_time))\n",
    "        num_present += is_present\n",
    "        if is_present:\n",
    "            glitch_gps_times_in_testset.append(glitch_time)\n",
    "            glitch_durations_in_testset.append(gdur)\n",
    "            glitch_snrs_in_testset.append(gsnr)\n",
    "            glitch_types_in_testset.append(gtype)\n",
    "    print('There were {} glitches present in the testing data for H1 O3a'.format(num_present))\n",
    "    return (glitch_gps_times_in_testset, glitch_durations_in_testset, glitch_snrs_in_testset, glitch_types_in_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nclosest_index_for_glitch(trigger_times, injection_times, glitch_duration):\n",
    "    # Find nclosest triggers given an injection time\n",
    "    if len(trigger_times) == 0:\n",
    "        raise ValueError('Cannot find closest index for empty trigger times array.')\n",
    "    \n",
    "    # Sort array before proceeding\n",
    "    array = trigger_times.copy()\n",
    "    array.sort()\n",
    "    \n",
    "    # Find nclosest indices for the given value in array\n",
    "    tidxs = []\n",
    "    for inj, gdur in zip(injection_times, glitch_duration):\n",
    "        # tidxs.append(np.argwhere((array >= inj + (-1*gdur)) & (array <= inj + gdur)))\n",
    "        tidxs.append(np.argwhere((array >= inj) & (array <= inj+gdur)))\n",
    "    return tidxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How close was the peak glitch time to a GW injection\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return abs(array[idx] - value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_trigger = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_glitches(tname, glitch_gps_times_in_testset, glitch_duration, glitch_snrs_in_testset, glitch_types_in_testset):\n",
    "    # Get the event time for glitches and get triggers around this GPS time\n",
    "    # This procedure is similar to what we already have with the ROC curve code\n",
    "    # Plot all network outputs without maximising against all noise triggers\n",
    "    bg_events = teams[tname]['bg_events']\n",
    "\n",
    "    \"\"\" Triggers around glitch time \"\"\"\n",
    "    # Read the background output and get all the trigger times found\n",
    "    bg_trigger_times = bg_events[0]\n",
    "    bg_trigger_stats = bg_events[1]\n",
    "\n",
    "    # Given all the valid injection times present in the injections file\n",
    "    # get idx of all triggers that are within a bound of the given injection time\n",
    "    print(\"Getting closest trigger indices for each valid injection time\")\n",
    "    bg_bound_triggers_idx = find_nclosest_index_for_glitch(trigger_times=bg_trigger_times, injection_times=glitch_gps_times_in_testset, glitch_duration=glitch_duration)\n",
    "\n",
    "    foo = team_1 if tname=='Sage' else team_2\n",
    "    thresh_vals = dict(zip(['1-per-month', '1-per-week', '1-per-day', '100-per-month', '1000-per-month'], foo['far_thresholds']))\n",
    "    print(thresh_vals)\n",
    "\n",
    "    # Get glitch data\n",
    "    glitch_stat_snr = []\n",
    "    for btidx, gsnr, gtime, gtype in zip(bg_bound_triggers_idx, glitch_snrs_in_testset, glitch_gps_times_in_testset, glitch_types_in_testset):\n",
    "        if not single_trigger:\n",
    "            triggers = bg_trigger_stats[btidx]\n",
    "            glitch_data = np.column_stack((triggers, np.full(len(triggers), gsnr), np.full(len(triggers), gtime), np.full(len(triggers), gtype)))\n",
    "        else:\n",
    "            if len(btidx) != 0:\n",
    "                triggers = [np.max(bg_trigger_stats[btidx])]\n",
    "            else:\n",
    "                triggers = bg_trigger_stats[btidx]\n",
    "            glitch_data = np.column_stack((triggers, np.full(len(triggers), gsnr), np.full(len(triggers), gtime), np.full(len(triggers), gtype)))\n",
    "        glitch_stat_snr.append(glitch_data)\n",
    "\n",
    "    # Combine all triggers\n",
    "    glitch_stat_snr = np.row_stack((glitch_stat_snr))\n",
    "    # Pick glitch stats that are above the FAR threshold for 1000/month\n",
    "    ridxs = np.argwhere((glitch_stat_snr[:,0] > thresh_vals['1000-per-month']) & (glitch_types_in_testset != 'No_Glitch')).flatten()\n",
    "    # ridxs = np.argwhere(glitch_stat_snr[:,0] > thresh_vals['1000-per-month']).flatten()\n",
    "    # glitches_above_1000_per_month = glitch_stat_snr[ridxs]\n",
    "    glitches_above_1000_per_month = glitch_stat_snr # use this for violin plot stuff\n",
    "\n",
    "    # glitches_above_1000_per_month = glitch_stat_snr\n",
    "    print('Number of glitches above an FAR of 1000/month = {}'.format(glitches_above_1000_per_month.shape[0]))\n",
    "\n",
    "    nearest_event_dt = []\n",
    "    for gtime in glitches_above_1000_per_month[:,2]:\n",
    "        nearest_event_dt.append(find_nearest(team_1['tc'], gtime))\n",
    "    \n",
    "    return (glitches_above_1000_per_month, thresh_vals, nearest_event_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detfiles = ['H1_O3a', 'L1_O3a']\n",
    "team_names = ['Sage_ub', 'Sage', 'PyCBC']\n",
    "coords = itertools.product(team_names, detfiles)\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(13.0*2, 9.0*3))\n",
    "axs = axs.flatten()\n",
    "\n",
    "save_glitch_data = {'Sage':{'H1_O3a':None, 'L1_O3a':None}, 'PyCBC':{'H1_O3a':None, 'L1_O3a':None}}\n",
    "tmp = {'Sage_ub':{'H1_O3a':None, 'L1_O3a':None}}\n",
    "\n",
    "for num, ((tname, detfile), ax) in enumerate(zip(coords, axs)):\n",
    "    print('\\nCurrent coord: {}, {}'.format(detfile, tname))\n",
    "    if tname == 'Sage_ub':\n",
    "        ax.set_ylabel('Ranking Statistic ({}, {})'.format('Sage', detfile.replace('_', ' ')), fontsize=22)\n",
    "    else:\n",
    "        ax.set_ylabel('Ranking Statistic ({}, {})'.format(tname, detfile.replace('_', ' ')), fontsize=22)\n",
    "    ax.set_xlabel('Glitch SNR', fontsize=22)\n",
    "        \n",
    "    if tname == 'Sage_ub':\n",
    "        with open('thresh_vals_{}.pickle'.format(detfile), 'rb') as handle:\n",
    "            thresh_vals = pickle.load(handle)\n",
    "        with open('xmin_{}.pickle'.format(detfile), 'rb') as handle:\n",
    "            xmin = pickle.load(handle)\n",
    "        with open('xmax_{}.pickle'.format(detfile), 'rb') as handle:\n",
    "            xmax = pickle.load(handle)\n",
    "        with open('glitch_above_1000_{}.pickle'.format(detfile), 'rb') as handle:\n",
    "            glitches_above_1000_per_month = pickle.load(handle)\n",
    "\n",
    "        for thresh_name, thresh in thresh_vals.items():\n",
    "            ax.hlines(y=thresh, xmin=xmin, xmax=xmax, linewidth=2., label=thresh_name, colors='k', zorder=1)\n",
    "        # Scatter plot of glitch triggers\n",
    "        foo = ax.scatter(glitches_above_1000_per_month[:,1], glitches_above_1000_per_month[:,0], s=256, alpha=0.8, c=\"#65156E\", edgecolors='k')\n",
    "\n",
    "        # placing text instead of legend\n",
    "        for thresh_name, tval in thresh_vals.items():\n",
    "            factor = 0.9 if detfile == 'L1_O3a' else 0.8\n",
    "            ax.text(factor*xmax, tval+0.05, '{}'.format(thresh_name), horizontalalignment='right', fontsize=14)\n",
    "        \n",
    "        ax.grid(which='both')\n",
    "        ax.set_xlim(xmin, xmax)\n",
    "        ax.set_xscale('log')\n",
    "        ax.tick_params(axis='both', which='major', labelsize=22)\n",
    "        ax.tick_params(axis='both', which='minor', labelsize=22)\n",
    "\n",
    "        tmp[tname][detfile] = glitches_above_1000_per_month\n",
    "        save_glitch_data.update(tmp)\n",
    "\n",
    "        continue\n",
    "    \n",
    "    glitch_gps_times, glitch_duration, glitch_snrs, glitch_types = get_glitch_data(detfile)\n",
    "    glitch_gps_times_in_testset, glitch_durations_in_testset, glitch_snrs_in_testset, glitch_types_in_testset = glitches_in_testset(glitch_gps_times, glitch_duration, glitch_snrs, glitch_types)\n",
    "    glitches_above_1000_per_month, thresh_vals, nearest_event_dt = get_relevant_glitches(tname, glitch_gps_times_in_testset, glitch_duration, glitch_snrs_in_testset, glitch_types_in_testset)\n",
    "\n",
    "    # Save glitch data\n",
    "    save_glitch_data[tname][detfile] = glitches_above_1000_per_month\n",
    "\n",
    "    # Plot all glitch triggers alongside the thresh vals\n",
    "    xmin = min(glitches_above_1000_per_month[:,1]) - 0.1*min(glitches_above_1000_per_month[:,1])\n",
    "    xmax = max(glitches_above_1000_per_month[:,1]) + 0.1*max(glitches_above_1000_per_month[:,1])\n",
    "    xmin = 5.0\n",
    "    xmax = 10_000\n",
    "    n = len(thresh_vals.keys())\n",
    "\n",
    "    # Threshold lines\n",
    "    colors = np.array([raspberry, dark_red, dark_blue, dark_green, dark_purple])[::-1]\n",
    "    for (thresh_name, thresh), c in zip(thresh_vals.items(), colors):\n",
    "        ax.hlines(y=thresh, xmin=xmin, xmax=xmax, linewidth=2., label=thresh_name, colors='k', zorder=1)\n",
    "    # Scatter plot of glitch triggers\n",
    "    # foo = ax.scatter(glitches_above_1000_per_month[:,1], glitches_above_1000_per_month[:,0], s=256, alpha=0.8, c=nearest_event_dt, cmap='viridis_r', edgecolors='k')\n",
    "    foo = ax.scatter(glitches_above_1000_per_month[:,1], glitches_above_1000_per_month[:,0], s=256, alpha=0.8, c=\"#65156E\", edgecolors='k')\n",
    "\n",
    "    # placing text instead of legend\n",
    "    for thresh_name, tval in thresh_vals.items():\n",
    "        factor = 0.9 if detfile == 'L1_O3a' else 0.8\n",
    "        # ax.text(factor*max(glitches_above_1000_per_month[:,1]), tval+0.05, '{}'.format(thresh_name), horizontalalignment='right', fontsize=14)\n",
    "        ax.text(factor*xmax, tval+0.05, '{}'.format(thresh_name), horizontalalignment='right', fontsize=14)\n",
    "\n",
    "\n",
    "    if tname == 'Sage' and False:\n",
    "        with open('thresh_vals_{}.pickle'.format(detfile), 'wb') as handle:\n",
    "            pickle.dump(thresh_vals, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open('xmin_{}.pickle'.format(detfile), 'wb') as handle:\n",
    "            pickle.dump(xmin, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open('xmax_{}.pickle'.format(detfile), 'wb') as handle:\n",
    "            pickle.dump(xmax, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open('glitch_above_1000_{}.pickle'.format(detfile), 'wb') as handle:\n",
    "            pickle.dump(glitches_above_1000_per_month, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    \"\"\"\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    cbar = fig.colorbar(foo, cax=cax, orientation='vertical', alpha=1.0)\n",
    "    if num == 1 or num == 3 or True:\n",
    "        cbar.set_label(r'$\\mathregular{min|T^{g} - T^{inj}_i|}$', rotation=90)\n",
    "        cbar.set_alpha(1.0)\n",
    "    \"\"\"\n",
    "    ax.grid(which='both')\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.set_xscale('log')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=22)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=22)\n",
    "\n",
    "save_path = \"./evaluation_plots/glitch_triggers.png\"\n",
    "# plt.tight_layout()\n",
    "left  = 0.05    # the left side of the subplots of the figure\n",
    "right = 0.1    # the right side of the subplots of the figure\n",
    "bottom = 0.1   # the bottom of the subplots of the figure\n",
    "top = 0.1      # the top of the subplots of the figure\n",
    "wspace = 0.2   # the amount of width reserved for blank space between subplots\n",
    "hspace = 0.2   # the amount of height reserved for white space between subplots\n",
    "plt.subplots_adjust(wspace=wspace)\n",
    "plt.savefig(save_path, bbox_inches='tight', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigger, snr, gps_time, type\n",
    "save_glitch_data['Sage']['H1_O3a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make segments file for O3b\n",
    "H1_O3b_csv = pd.read_csv('./O3b/H1_O3b_GPS.txt', names=['start', 'end', 'duration'], header=None, delimiter=' ')\n",
    "L1_O3b_csv = pd.read_csv('./O3b/L1_O3b_GPS.txt', names=['start', 'end', 'duration'], header=None, delimiter=' ')\n",
    "H1_O3b_csv.to_csv('./O3b/H1_O3b_GPS.csv', sep=',', encoding='utf-8', header=True)\n",
    "L1_O3b_csv.to_csv('./O3b/L1_O3b_GPS.csv', sep=',', encoding='utf-8', header=True)\n",
    "print(H1_O3b_csv)\n",
    "print(L1_O3b_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glitch_gps_times, glitch_duration, glitch_snrs, glitch_types = get_glitch_data('H1_O3a')\n",
    "_, _, _, glitch_types_in_testset_H1_O3a = glitches_in_testset(glitch_gps_times, glitch_duration, glitch_snrs, glitch_types)\n",
    "glitch_gps_times, glitch_duration, glitch_snrs, glitch_types = get_glitch_data('L1_O3a')\n",
    "_, _, _, glitch_types_in_testset_L1_O3a = glitches_in_testset(glitch_gps_times, glitch_duration, glitch_snrs, glitch_types)\n",
    "glitch_gps_times, glitch_duration, glitch_snrs, glitch_types = get_glitch_data('H1_O3b')\n",
    "_, _, _, glitch_types_in_testset_H1_O3b = glitches_in_testset(glitch_gps_times, glitch_duration, glitch_snrs, glitch_types, segments_csv='./O3b/H1_O3b_GPS.csv')\n",
    "glitch_gps_times, glitch_duration, glitch_snrs, glitch_types = get_glitch_data('L1_O3b')\n",
    "_, _, _, glitch_types_in_testset_L1_O3b = glitches_in_testset(glitch_gps_times, glitch_duration, glitch_snrs, glitch_types, segments_csv='./O3b/L1_O3b_GPS.csv')\n",
    "\n",
    "print(glitch_types_in_testset_H1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_glitches = glitch_types_in_testset_H1_O3a + glitch_types_in_testset_H1_O3b + glitch_types_in_testset_L1_O3a + glitch_types_in_testset_L1_O3b\n",
    "unique, counts = np.unique(all_glitches, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnt = 26\n",
    "\n",
    "# create test data\n",
    "team_names = ['Sage_ub', 'Sage', 'PyCBC']\n",
    "detfiles = ['H1_O3a', 'L1_O3a']\n",
    "coords = itertools.product(team_names, detfiles)\n",
    "\n",
    "# figsize - (width, height)\n",
    "# subplots - (nrows, ncols)\n",
    "fig = plt.figure(figsize=(10.0*3, 7.5*4), constrained_layout=True)\n",
    "gs = fig.add_gridspec(4, 3, width_ratios=[2, 2, 1], height_ratios=[2, 2, 2, 2], \n",
    "                      wspace=0.03, hspace=0.04)\n",
    "\n",
    "## Glitch type frequency bar plot for training data\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "ax1.bar(glitch_alias, counts, width=1.0, color='#025196', edgecolor='k', linewidth=1)\n",
    "ax1.set_ylabel('N Glitches in Training Data', fontsize=fnt)\n",
    "ax1.tick_params(labelsize=24)\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_axisbelow(True)\n",
    "ax1.yaxis.grid(color='gray', linestyle='dashed', which='major', linewidth=2.0)\n",
    "ax1.yaxis.grid(color='gray', linestyle='dashed', which='minor', linewidth=0.5)\n",
    "\n",
    "## Glitch type acronyms and legend for swarmplot\n",
    "ax2 = fig.add_subplot(gs[1:, -1])\n",
    "\n",
    "for n, (galias, gname) in enumerate(zip(glitch_alias, glitch_names)):\n",
    "    ax2.text(0.05, 0.75-n*0.03, '{} = {}'.format(galias, gname), horizontalalignment='left', fontsize=fnt)\n",
    "\n",
    "sage_patch = mpatches.Patch(color='#512888', label='Sage')\n",
    "pycbc_patch = mpatches.Patch(color='#EB6123', label='PyCBC')\n",
    "# left/right, up/down, width, height\n",
    "plt.legend(handles=[sage_patch, pycbc_patch], bbox_to_anchor=(0.15, -2.4, 1., 6.5), loc='center left', frameon=False, fontsize=28, handlelength=2.6)\n",
    "# Remove borders and ticks\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "for pos in ['right', 'top', 'bottom', 'left']: \n",
    "    plt.gca().spines[pos].set_visible(False) \n",
    "\n",
    "\n",
    "## Top glitches comparison\n",
    "topgs = []\n",
    "topgs.append(fig.add_subplot(gs[1, 0]))\n",
    "topgs.append(fig.add_subplot(gs[1, 1]))\n",
    "topgs.append(fig.add_subplot(gs[2, 0]))\n",
    "topgs.append(fig.add_subplot(gs[2, 1]))\n",
    "topgs.append(fig.add_subplot(gs[3, 0]))\n",
    "topgs.append(fig.add_subplot(gs[3, 1]))\n",
    "\n",
    "\n",
    "for num, ((tname, detfile), ax) in enumerate(zip(coords, topgs)):\n",
    "    print('\\nCurrent coord: {}, {}'.format(detfile, tname))\n",
    "\n",
    "    if tname in ['Sage']:\n",
    "        team_data = team_1\n",
    "        team_name = tname\n",
    "        alt_team_data = team_2\n",
    "        alt_team_name = 'PyCBC'\n",
    "    elif tname in ['Sage_ub']:\n",
    "        with open('noise_stats.pickle', 'rb') as handle:\n",
    "            noise_stats = pickle.load(handle)\n",
    "        team_data = {'noise_stats': noise_stats}\n",
    "        team_name = tname\n",
    "        alt_team_data = team_2\n",
    "        alt_team_name = 'PyCBC'\n",
    "    elif tname in ['PyCBC']:\n",
    "        team_data = team_2\n",
    "        team_name = tname\n",
    "        with open('noise_stats.pickle', 'rb') as handle:\n",
    "            noise_stats = pickle.load(handle)\n",
    "        alt_team_data = {'noise_stats': noise_stats}\n",
    "        alt_team_name = 'Sage_ub'\n",
    "    \n",
    "    print(team_name, alt_team_name)\n",
    "    # Convert triggers to FAR/month\n",
    "    team_stats_in_far = [np.log10(len(team_data['noise_stats'][team_data['noise_stats'] > foo])) for foo in save_glitch_data[team_name][detfile][:,0]]\n",
    "    team_triggers = np.column_stack((save_glitch_data[team_name][detfile][:,3], team_stats_in_far))\n",
    "    # alt_team\n",
    "    alt_team_stats_in_far = [np.log10(len(alt_team_data['noise_stats'][alt_team_data['noise_stats'] > foo])) for foo in save_glitch_data[alt_team_name][detfile][:,0]]\n",
    "    alt_team_triggers = np.column_stack((save_glitch_data[alt_team_name][detfile][:,3], alt_team_stats_in_far))\n",
    "\n",
    "    # Iterate through all glitch types\n",
    "    save_team_triggers = {}\n",
    "    save_team_triggers_nums = {}\n",
    "    save_alt_team_triggers = {}\n",
    "    save_alt_team_triggers_nums = {}\n",
    "    for i in range(len(glitch_names)):\n",
    "        # Get all triggers for a particular glitch type\n",
    "        save_team_triggers[i] = team_triggers[:,1][team_triggers[:,0] == i]\n",
    "        save_alt_team_triggers[i] = alt_team_triggers[:,1][alt_team_triggers[:,0] == i]\n",
    "        # glitch_type, trigger_value\n",
    "        try: \n",
    "            save_team_triggers_nums[i] = np.min(team_triggers[:,1][team_triggers[:,0] == i])\n",
    "        except: \n",
    "            pass\n",
    "        # Alt team\n",
    "        try: \n",
    "            save_alt_team_triggers_nums[i] = np.min(alt_team_triggers[:,1][alt_team_triggers[:,0] == i]) \n",
    "        except: \n",
    "            pass\n",
    "\n",
    "    # Which triggers types have the highest trigger values\n",
    "    highest_team_triggers = dict(sorted(save_team_triggers_nums.items(), key=lambda item: item[1], reverse=False))\n",
    "    print(highest_team_triggers)\n",
    "    highest_alt_team_triggers = dict(sorted(save_alt_team_triggers_nums.items(), key=lambda item: item[1], reverse=False))\n",
    "    top10_glitches_team = list(highest_team_triggers.keys())[:3]\n",
    "    top10_glitches_alt_team = top10_glitches_team.copy()\n",
    "\n",
    "    team_data = [save_team_triggers[i][save_team_triggers[i] < 5] for i in top10_glitches_team]\n",
    "    team_swarm_data = [save_team_triggers[i][save_team_triggers[i] < 4] for i in top10_glitches_team]\n",
    "    alt_team_data = [save_alt_team_triggers[i][save_alt_team_triggers[i] < 5] for i in top10_glitches_alt_team]\n",
    "    alt_team_swarm_data = [save_alt_team_triggers[i][save_alt_team_triggers[i] < 4] for i in top10_glitches_alt_team]\n",
    "\n",
    "    # Make violin plot using searborn\n",
    "    team_labels = [names_to_alias[map_to_names[foo]] for foo in top10_glitches_team]\n",
    "    alt_team_labels = [names_to_alias[map_to_names[foo]] for foo in top10_glitches_alt_team]\n",
    "\n",
    "    # Team data\n",
    "    team_violin_data = {key:val for key, val in zip(team_labels, team_data)}\n",
    "    team_swarm_datadict = {key:val for key, val in zip(team_labels, team_swarm_data)}\n",
    "    # Alt team data\n",
    "    alt_team_violin_data = {key:val for key, val in zip(alt_team_labels, alt_team_data)}\n",
    "    alt_team_swarm_datadict = {key:val for key, val in zip(alt_team_labels, alt_team_swarm_data)}\n",
    "\n",
    "    df = pd.DataFrame([[label, val, team_name] for label, values in team_violin_data.items() for val in values]\n",
    "                    + [[label, val, alt_team_name] for label, values in alt_team_violin_data.items() for val in values],\n",
    "                    columns=['Glitch Types ({}, {})'.format(team_name, detfile.replace('_', ' ')), r'$\\mathregular{log}_{\\mathregular{10}}(\\mathregular{FAR/month})$', 'source'])\n",
    "\n",
    "    swarmdf = pd.DataFrame([[label, val, team_name] for label, values in team_swarm_datadict.items() for val in values]\n",
    "                    + [[label, val, alt_team_name] for label, values in alt_team_swarm_datadict.items() for val in values],\n",
    "                    columns=['Glitch Types ({}, {})'.format(team_name, detfile.replace('_', ' ')), r'$\\mathregular{log}_{\\mathregular{10}}(\\mathregular{FAR/month})$', 'source'])\n",
    "\n",
    "    # ax = sns.violinplot(df, cut=0, orient='h', y='label', x='value', hue='source', inner='quart')\n",
    "    colors = ['#512888', '#EB6123'] if team_name in ['Sage', 'Sage_ub'] else ['#EB6123', '#512888']\n",
    "    label_team_name = 'Sage' if team_name in ['Sage', 'Sage_ub'] else 'PyCBC'\n",
    "    swarm = sns.swarmplot(swarmdf, dodge=False, legend=False, ax=ax, orient='h', y='Glitch Types ({}, {})'.format(team_name, detfile.replace('_', ' ')), \n",
    "                  x=r'$\\mathregular{log}_{\\mathregular{10}}(\\mathregular{FAR/month})$', size=10, palette=colors, hue='source')\n",
    "\n",
    "    # Thesholds in FAR\n",
    "    colors = np.array([raspberry, dark_red, dark_blue, dark_green, dark_purple])[::-1]\n",
    "    thresh_names = ['1-per-month', '10-per-month', '100-per-month', '1000-per-month', '10000-per-month']\n",
    "    thresh_vals = [np.log10(1), np.log10(10), np.log10(100), np.log10(1000), np.log10(10000)]\n",
    "    thresh = {name:val for name, val in zip(thresh_names, thresh_vals)}\n",
    "    for (thresh_name, thresh), c in zip(thresh.items(), colors):\n",
    "        ax.axvline(thresh, ls='--', linewidth=1., label=thresh_name, color='k')\n",
    "    \n",
    "    if team_name == 'Sage_ub':\n",
    "        team_name = 'Sage'\n",
    "    \n",
    "    ax.set_xlabel(r'$\\mathregular{log}_{\\mathregular{10}}(\\mathregular{FAR/month})$', fontsize=fnt)\n",
    "    ax.set_ylabel('Glitch Types ({}, {})'.format(team_name, detfile.replace('_', ' ')), fontsize=fnt)\n",
    "    ax.tick_params(labelsize=fnt)\n",
    "\n",
    "    ax = swarm.axes\n",
    "    ax.invert_xaxis()\n",
    "    ax.grid(axis='x')\n",
    "\n",
    "plt.savefig('./evaluation_plots/glitch_swarmplot_pycbc_sage.png')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robustness to PSD realisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "from pycbc.conversions import mchirp_from_mass1_mass2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_vout(gs, vout_1, vout_2, limits=[[-999, +999], [-999, +999]], labels=[\"\", \"\"]):\n",
    "    vdata_1 = {}\n",
    "    vdata_2 = {}\n",
    "\n",
    "    with h5py.File(vout_1, 'r') as fp:\n",
    "        keys = list(fp.keys())\n",
    "        for key in keys:\n",
    "            vdata_1[key] = fp[key][()]\n",
    "\n",
    "    with h5py.File(vout_2, 'r') as fp:\n",
    "        keys = list(fp.keys())\n",
    "        for key in keys:\n",
    "            vdata_2[key] = fp[key][()]\n",
    "    \n",
    "    cell_1 = gs[0]\n",
    "    cell_2 = gs[1]\n",
    "\n",
    "    ### CELL 1 ###\n",
    "    idxs_1 = np.argwhere(vdata_1['epoch_labelsgw'].astype(np.int64) == 0).flatten()\n",
    "    idxs_2 = np.argwhere(vdata_2['epoch_labelsgw'].astype(np.int64) == 0).flatten()\n",
    "    compare_1 = vdata_1['raw_output'][idxs_1]\n",
    "    compare_2 = vdata_2['raw_output'][idxs_2]\n",
    "\n",
    "    nbins = 1024\n",
    "    bins = np.histogram(np.hstack((compare_1, compare_2)), bins=nbins)[1]\n",
    "\n",
    "    ax_1 = fig.add_subplot(cell_1)\n",
    "    # Trainnet on test\n",
    "    ax_1.hist(compare_1, bins=bins, histtype='step', color='blue', linewidth=0.5, density=True)\n",
    "    sns.kdeplot(data=compare_1, cut=3, gridsize=512, label=labels[0], ax=ax_1)\n",
    "    # Trainnet on train\n",
    "    ax_1.hist(compare_2, bins=bins, histtype='step', color='orange', linewidth=0.5, density=True)\n",
    "    sns.kdeplot(data=compare_2, cut=3, gridsize=512, label=labels[1], ax=ax_1)\n",
    "\n",
    "    # We change the fontsize of minor ticks label \n",
    "    ax_1.tick_params(axis='both', which='major', labelsize=16)\n",
    "    ax_1.tick_params(axis='both', which='minor', labelsize=16)\n",
    "    ax_1.set_xlim(limits[0][0], limits[0][1])\n",
    "    ax_1.set_ylabel('Density', fontsize=16)\n",
    "\n",
    "    ### CELL 2 ###\n",
    "    idxs_1 = np.nonzero(vdata_1['epoch_labelsgw'].astype(np.int64))\n",
    "    idxs_2 = np.nonzero(vdata_2['epoch_labelsgw'].astype(np.int64))\n",
    "    compare_1 = vdata_1['raw_output'][idxs_1]\n",
    "    compare_2 = vdata_2['raw_output'][idxs_2]\n",
    "\n",
    "    nbins = 1024\n",
    "    bins = np.histogram(np.hstack((compare_1, compare_2)), bins=nbins)[1]\n",
    "\n",
    "    ax_2 = fig.add_subplot(cell_2)\n",
    "    # Trainnet on test\n",
    "    ax_2.hist(compare_1, bins=bins, histtype='step', color='blue', linewidth=0.5, density=True)\n",
    "    sns.kdeplot(data=compare_1, cut=3, gridsize=512, label=labels[0], ax=ax_2)\n",
    "    # Trainnet on train\n",
    "    ax_2.hist(compare_2, bins=bins, histtype='step', color='orange', linewidth=0.5, density=True)\n",
    "    sns.kdeplot(data=compare_2, cut=3, gridsize=512, label=labels[1], ax=ax_2)\n",
    "\n",
    "    # We change the fontsize of minor ticks label \n",
    "    ax_2.tick_params(axis='both', which='major', labelsize=16)\n",
    "    ax_2.tick_params(axis='both', which='minor', labelsize=16)\n",
    "    ax_2.set_xlim(limits[1][0], limits[1][1])\n",
    "    ax_2.set_ylabel('Density', fontsize=16)\n",
    "\n",
    "    # hide the spines between ax and ax2\n",
    "    ax_1.spines['right'].set_linestyle((0,(4,4)))\n",
    "    ax_2.spines['left'].set_linestyle((0,(4,4)))\n",
    "    ax_1.yaxis.tick_left()\n",
    "    ax_2.yaxis.tick_right()\n",
    "    ax_2.yaxis.set_label_position(\"right\")\n",
    "    ax_1.grid(True)\n",
    "    ax_2.grid(True)\n",
    "    ax_2.legend(fontsize=14)\n",
    "\n",
    "    plt.setp(ax_1.get_xticklabels()[-1], visible=False)    \n",
    "    plt.setp(ax_2.get_xticklabels()[0], visible=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 1024\n",
    "\n",
    "# Make outer gridspec\n",
    "fig = plt.figure(figsize=(10.0*2, 5.0*5))\n",
    "plt.subplots_adjust(wspace=0.01)\n",
    "outer = fig.add_gridspec(5, 1, height_ratios=[2, 2, 2, 2, 2])\n",
    "# Make nested gridspecs\n",
    "gs1 = gridspec.GridSpecFromSubplotSpec(1, 2, subplot_spec = outer[0])\n",
    "gs2 = gridspec.GridSpecFromSubplotSpec(1, 2, subplot_spec = outer[1])\n",
    "gs3 = gridspec.GridSpecFromSubplotSpec(1, 2, subplot_spec = outer[2])\n",
    "gs4 = gridspec.GridSpecFromSubplotSpec(1, 2, subplot_spec = outer[3])\n",
    "gs5 = gridspec.GridSpecFromSubplotSpec(1, 2, subplot_spec = outer[4])\n",
    "\n",
    "# D3net on training dataset && testing dataset\n",
    "print('D3net on training dataset && testing dataset')\n",
    "vout_1 = \"/home/nnarenraju/Research/ORChiD/RUNS/Dataset3_1epoch_validation_Sept8_traindata/validation_output.hdf\"\n",
    "vout_2 = \"/home/nnarenraju/Research/ORChiD/RUNS/Dataset3_1epoch_validation_Sept8/validation_output.hdf\"\n",
    "compare_vout(gs1, vout_1, vout_2, limits=[[-6, 4], [10, 15]], labels=['D3Net on train dataset', 'D3Net on testing dataset'])\n",
    "\n",
    "# Trainnet and Testnet on training data\n",
    "print('Trainnet and Testnet on training data')\n",
    "vout_1 = \"/home/nnarenraju/Research/ORChiD/RUNS/TrainRecolour_1epoch_validation_Sept7_traindata/validation_output.hdf\"\n",
    "vout_2 = \"/home/nnarenraju/Research/ORChiD/RUNS/TestRecolour_1epoch_validation_Sept8_traindata/validation_output.hdf\"\n",
    "compare_vout(gs2, vout_1, vout_2, limits=[[-6, 4], [10, 15]], labels=['TrainNet on train dataset', 'TestNet on train dataset'])\n",
    "\n",
    "# Trainnet && Testnet on testing dataset\n",
    "print('Trainnet && Testnet on testing dataset')\n",
    "vout_1 = \"/home/nnarenraju/Research/ORChiD/RUNS/TrainRecolour_1epoch_validation_Sept7/validation_output.hdf\"\n",
    "vout_2 = \"/home/nnarenraju/Research/ORChiD/RUNS/TestRecolour_1epoch_validation_Sept8/validation_output.hdf\"\n",
    "compare_vout(gs3, vout_1, vout_2, limits=[[-6, 4], [10, 15]], labels=['TrainNet on test dataset ', 'TestNet on test dataset'])\n",
    "\n",
    "# Trainnet on training dataset && testing dataset\n",
    "print('Trainnet on training dataset && testing dataset')\n",
    "vout_1 = \"/home/nnarenraju/Research/ORChiD/RUNS/TrainRecolour_1epoch_validation_Sept7_traindata/validation_output.hdf\"\n",
    "vout_2 = \"/home/nnarenraju/Research/ORChiD/RUNS/TrainRecolour_1epoch_validation_Sept7/validation_output.hdf\"\n",
    "compare_vout(gs4, vout_1, vout_2, limits=[[-6, 4], [10, 15]], labels=['TrainNet on train dataset', 'TrainNet on test dataset'])\n",
    "\n",
    "# Testnet on training dataset && testing dataset\n",
    "print('Testnet on training dataset && testing dataset')\n",
    "vout_1 = \"/home/nnarenraju/Research/ORChiD/RUNS/TestRecolour_1epoch_validation_Sept8_traindata/validation_output.hdf\"\n",
    "vout_2 = \"/home/nnarenraju/Research/ORChiD/RUNS/TestRecolour_1epoch_validation_Sept8/validation_output.hdf\"\n",
    "compare_vout(gs5, vout_1, vout_2, limits=[[-6, 4], [10, 15]], labels=['TestNet on train dataset', 'TestNet on test dataset'])\n",
    "\n",
    "plt.savefig('./evaluation_plots/psd_robustness.png', bbox_inches='tight', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norland = \"/home/nnarenraju/Research/ORChiD/RUNS/Norland_D3_template_placement_metric_Jul26/losses.txt\"\n",
    "sagenet = \"/home/nnarenraju/Research/ORChiD/RUNS/recent_runs/SageNet50_metric_density_Jun26/losses.txt\"\n",
    "biased = \"/home/nnarenraju/Research/ORChiD/RUNS/recent_runs/SageNet50_halfnormSNR_May17_BOY/losses.txt\"\n",
    "annealed = \"./annealed_losses.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norland_aux = pd.read_csv(norland, sep=\"    \", engine='python')\n",
    "sagenet_aux = pd.read_csv(sagenet, sep=\"    \", engine='python')\n",
    "biased_aux = pd.read_csv(biased, sep=\"    \", engine='python')\n",
    "annealed_aux = pd.read_csv(annealed, sep=\"    \", engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 or 1 - mchirp = [min, 25.0]\n",
    "# 2 or 3 - mchirp = [25.0, max]\n",
    "# 0 or 2 - SNR = 5\n",
    "# 1 or 3 - SNR = 12\n",
    "\n",
    "norland_epochs = norland_aux['epoch']\n",
    "norland_low_snr_low_mchirp = norland_aux['aux_0']\n",
    "norland_low_snr_high_mchirp = norland_aux['aux_2']\n",
    "norland_high_snr_low_mchirp = norland_aux['aux_1']\n",
    "norland_high_snr_high_mchirp = norland_aux['aux_3']\n",
    "\n",
    "sagenet_epochs = sagenet_aux['epoch']\n",
    "sagenet_low_snr_low_mchirp = sagenet_aux['aux_0']\n",
    "sagenet_low_snr_high_mchirp = sagenet_aux['aux_2']\n",
    "sagenet_high_snr_low_mchirp = sagenet_aux['aux_1']\n",
    "sagenet_high_snr_high_mchirp = sagenet_aux['aux_3']\n",
    "\n",
    "biased_epochs = biased_aux['epoch']\n",
    "biased_low_snr_low_mchirp = biased_aux['aux_0']\n",
    "biased_low_snr_high_mchirp = biased_aux['aux_2']\n",
    "biased_high_snr_low_mchirp = biased_aux['aux_1']\n",
    "biased_high_snr_high_mchirp = biased_aux['aux_3']\n",
    "\n",
    "annealed_epochs = annealed_aux['epoch']\n",
    "annealed_low_snr_low_mchirp = annealed_aux['aux_0']\n",
    "annealed_low_snr_high_mchirp = annealed_aux['aux_2']\n",
    "annealed_high_snr_low_mchirp = annealed_aux['aux_1']\n",
    "annealed_high_snr_high_mchirp = annealed_aux['aux_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10.0, 8.0))\n",
    "\n",
    "### NORLAND\n",
    "yhat = savgol_filter(norland_low_snr_low_mchirp, 20, 3)\n",
    "plt.plot(norland_epochs, yhat, color='purple', linewidth=4.0, marker='o')\n",
    "yhat = savgol_filter(norland_low_snr_high_mchirp, 20, 3)\n",
    "plt.plot(norland_epochs, yhat, color='orange', linewidth=1.0, linestyle='dashed', marker='*')\n",
    "\n",
    "### SAGENET\n",
    "yhat = savgol_filter(sagenet_low_snr_low_mchirp, 20, 3)\n",
    "plt.plot(sagenet_epochs, yhat, color='red', linewidth=4.0, marker='o')\n",
    "yhat = savgol_filter(sagenet_low_snr_high_mchirp, 20, 3)\n",
    "plt.plot(sagenet_epochs, yhat, color='black', linewidth=1.0, linestyle='dashed', marker='*')\n",
    "\n",
    "### BIASED\n",
    "yhat = savgol_filter(biased_low_snr_low_mchirp, 20, 3)\n",
    "plt.plot(biased_epochs, yhat, color='blue', linewidth=4.0, marker='o')\n",
    "yhat = savgol_filter(biased_low_snr_high_mchirp, 20, 3)\n",
    "plt.plot(biased_epochs, yhat, color='pink', linewidth=1.0, linestyle='dashed', marker='*')\n",
    "\n",
    "### ANNEALED\n",
    "yhat = savgol_filter(annealed_low_snr_low_mchirp, 20, 3)\n",
    "plt.plot(annealed_epochs, yhat, color='green', linewidth=4.0, marker='o')\n",
    "yhat = savgol_filter(annealed_low_snr_high_mchirp, 20, 3)\n",
    "plt.plot(annealed_epochs, yhat, color='yellow', linewidth=1.0, linestyle='dashed', marker='*')\n",
    "\n",
    "plt.xlim(0, 60)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare template placement metric runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.lib.stride_tricks import sliding_window_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./annealed_data_sage_FAR1permonth_mchirp.pickle', 'rb') as handle:\n",
    "    mchirp_annealed = pickle.load(handle)\n",
    "with open('./metric_data_sage_FAR1permonth_mchirp.pickle', 'rb') as handle:\n",
    "    mchirp_metric = pickle.load(handle)\n",
    "with open('./data_pycbc_FAR1permonth_mchirp.pickle', 'rb') as handle:\n",
    "    mchirp_pycbc = pickle.load(handle)\n",
    "\n",
    "with open('./d3_metric_data_sage_FAR1permonth_mchirp.pickle', 'rb') as handle:\n",
    "    mchirp_metric_d3 = pickle.load(handle)\n",
    "with open('./d3_data_pycbc_FAR1permonth_mchirp.pickle', 'rb') as handle:\n",
    "    mchirp_pycbc_d3 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(9*2., 8*2.))\n",
    "plt.subplots_adjust(wspace=0.0)\n",
    "bins = np.histogram(np.hstack((mchirp_annealed, mchirp_metric, mchirp_pycbc)), bins=40)[1]\n",
    "ax[0][0].hist(mchirp_pycbc, density=False, bins=bins, alpha=0.8, histtype='step', color=colors_histogram[0], linewidth=2.0, linestyle='dashed', label='PyCBC')\n",
    "ax[0][0].hist(mchirp_annealed, density=False, bins=bins, alpha=0.8, histtype='step', color=colors_histogram[1], linewidth=2.0, linestyle='solid', label='Sage - Annealed Metric')\n",
    "ax[0][0].hist(mchirp_metric, density=False, bins=bins, alpha=0.8, histtype='step', color='#295e11', linewidth=2.0, linestyle='solid', label='Sage - Metric')\n",
    "ax[0][0].set_ylim(0.0, 180.0)\n",
    "ax[0][0].set_ylabel('N Detected Signals')\n",
    "ax[0][0].set_xlabel('Chirp Mass')\n",
    "ax[0][0].legend()\n",
    "\n",
    "bins = np.histogram(np.hstack((data_pycbc['mchirp'], mchirp_metric_d3)), bins=40)[1]\n",
    "ax[0][1].hist(mchirp_pycbc_d3, density=False, bins=bins, alpha=0.8, histtype='step', color=colors_histogram[0], linewidth=2.0, linestyle='dashed', label='PyCBC')\n",
    "ax[0][1].hist(mchirp_metric_d3, density=False, bins=bins, alpha=0.8, histtype='step', color=colors_histogram[1], linewidth=2.0, linestyle='solid', label='Sage - Metric')\n",
    "ax[0][1].set_yticklabels([])\n",
    "ax[0][1].set_yticks([])\n",
    "ax[0][1].set_ylim(0.0, 180.0)\n",
    "ax[0][1].set_xlabel('Chirp Mass')\n",
    "ax[0][1].legend()\n",
    "\n",
    "vout_1 = \"/home/nnarenraju/Research/ORChiD/RUNS/KennebecAnnealed_1epoch_validation_Sept7/validation_output.hdf\"\n",
    "vout_2 = \"/home/nnarenraju/Research/ORChiD/RUNS/MetricLatest_1epoch_validation_Sept7/validation_output.hdf\"\n",
    "vout_3 = \"/home/nnarenraju/Research/ORChiD/RUNS/Dataset3_1epoch_validation_Sept11_traindata/validation_output.hdf\"\n",
    "vout_data = [{}, {}, {}]\n",
    "for n, vout in enumerate([vout_1, vout_2, vout_3]):\n",
    "    with h5py.File(vout, 'r') as fp:\n",
    "        keys = list(fp.keys())\n",
    "        for key in keys:\n",
    "            vout_data[n][key] = fp[key][()]\n",
    "\n",
    "idxs_1 = np.argwhere(vout_data[0]['epoch_labelsgw'].astype(np.int64) == 0).flatten()\n",
    "idxs_2 = np.argwhere(vout_data[1]['epoch_labelsgw'].astype(np.int64) == 0).flatten()\n",
    "idxs_3 = np.argwhere(vout_data[2]['epoch_labelsgw'].astype(np.int64) == 0).flatten()\n",
    "noise_output_1 = vout_data[0]['raw_output'][idxs_1]\n",
    "noise_output_2 = vout_data[1]['raw_output'][idxs_2]\n",
    "noise_output_3 = vout_data[2]['raw_output'][idxs_3]\n",
    "\n",
    "idxs_1 = np.nonzero(vout_data[0]['epoch_labelsgw'].astype(np.int64))\n",
    "idxs_2 = np.nonzero(vout_data[1]['epoch_labelsgw'].astype(np.int64))\n",
    "idxs_3 = np.nonzero(vout_data[2]['epoch_labelsgw'].astype(np.int64))\n",
    "signal_output_1 = vout_data[0]['raw_output'][idxs_1]\n",
    "signal_output_2 = vout_data[1]['raw_output'][idxs_2]\n",
    "signal_output_3 = vout_data[2]['raw_output'][idxs_3]\n",
    "signal_mchirp_1 = vout_data[0]['sample_paramssignal_duration'][idxs_1]\n",
    "signal_mchirp_2 = vout_data[1]['sample_paramssignal_duration'][idxs_2]\n",
    "signal_mchirp_3 = vout_data[2]['sample_paramssignal_duration'][idxs_3]\n",
    "\n",
    "signal_data_1 = np.column_stack((signal_output_1, signal_mchirp_1))\n",
    "signal_data_2 = np.column_stack((signal_output_2, signal_mchirp_2))\n",
    "signal_data_3 = np.column_stack((signal_output_3, signal_mchirp_3))\n",
    "req_signal_data_1 = signal_data_1[signal_data_1[:,0] > max(noise_output_1)]\n",
    "req_signal_data_2 = signal_data_2[signal_data_2[:,0] > max(noise_output_2)]\n",
    "req_signal_data_3 = signal_data_3[signal_data_3[:,0] > max(noise_output_3)]\n",
    "req_signal_data_1 = req_signal_data_1[req_signal_data_1[:, 1].argsort()]\n",
    "req_signal_data_2 = req_signal_data_2[req_signal_data_2[:, 1].argsort()]\n",
    "req_signal_data_3 = req_signal_data_3[req_signal_data_3[:, 1].argsort()]\n",
    "\n",
    "# Smoothing and calculating error bars\n",
    "split = sliding_window_view(req_signal_data_1, 1000, axis=0)\n",
    "# Find mean of each chunk\n",
    "plot_data = np.array([[chunk[1][0], np.median(chunk[0]), np.percentile(chunk[0], 5), np.percentile(chunk[0], 95)] for chunk in split])\n",
    "# Plotting the above data for the given parameter\n",
    "ax[1][0].plot(plot_data[:,0], plot_data[:,1], label='Sage - Annealed Metric')\n",
    "ax[1][0].set_xlim(min(plot_data[:,0]), max(plot_data[:,0]))\n",
    "ax[1][0].fill_between(plot_data[:,0], plot_data[:,2], plot_data[:,3], color='blue', alpha = 0.2)\n",
    "\n",
    "# Smoothing and calculating error bars\n",
    "split = sliding_window_view(req_signal_data_2, 1000, axis=0)\n",
    "# Find mean of each chunk\n",
    "plot_data = np.array([[chunk[1][0], np.median(chunk[0]), np.percentile(chunk[0], 5), np.percentile(chunk[0], 95)] for chunk in split])\n",
    "# Plotting the above data for the given parameter\n",
    "ax[1][0].plot(plot_data[:,0], plot_data[:,1], label='Sage - Metric')\n",
    "ax[1][0].set_xlim(min(plot_data[:,0]), max(plot_data[:,0]))\n",
    "ax[1][0].fill_between(plot_data[:,0], plot_data[:,2], plot_data[:,3], color='red', alpha = 0.2)\n",
    "ax[1][0].set_ylim(8.0, 17.0)\n",
    "ax[1][0].set_xlabel('Chirp Mass')\n",
    "ax[1][0].set_ylabel('Network Output')\n",
    "ax[1][0].legend()\n",
    "\n",
    "# Smoothing and calculating error bars\n",
    "split = sliding_window_view(req_signal_data_3, 1000, axis=0)\n",
    "# Find mean of each chunk\n",
    "plot_data = np.array([[chunk[1][0], np.median(chunk[0]), np.percentile(chunk[0], 5), np.percentile(chunk[0], 95)] for chunk in split])\n",
    "# Plotting the above data for the given parameter\n",
    "ax[1][1].plot(plot_data[:,0], plot_data[:,1], label='Sage - Metric')\n",
    "ax[1][1].set_xlim(min(plot_data[:,0]), max(plot_data[:,0]))\n",
    "ax[1][1].fill_between(plot_data[:,0], plot_data[:,2], plot_data[:,3], color='blue', alpha = 0.2)\n",
    "ax[1][1].set_ylim(8.0, 17.0)\n",
    "ax[1][1].set_xlabel('Chirp Mass')\n",
    "ax[1][1].set_yticklabels([])\n",
    "ax[1][1].set_yticks([])\n",
    "ax[1][1].legend()\n",
    "\n",
    "plt.savefig('./evaluation_plots/compare_metric_runs.png', bbox_inches='tight', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
